
================================================================================
FILE: src/__init__.py
================================================================================

"""
File: src/__init__.py
Path: src/__init__.py
src package initialization
"""
__version__ = "0.1.0"



# # Core settings
# from src.settings import settings

# # Database components
# from src.db import Base, get_db, SessionLocal, engine, Document, get_documents, add_documents

# # Adapters
# from src.adapters import OpenAIEmbedder, SentenceTransformerEmbedder

# __all__ = [
#     "settings",
#     "Base", 
#     "get_db",
#     "SessionLocal",
#     "engine",
#     "Document",
#     "get_documents",
#     "add_documents",
#     "OpenAIEmbedder",
#     "SentenceTransformerEmbedder"
# ] 


================================================================================
FILE: src/adapters/__init__.py
================================================================================

"""
File: src/adapters/__init__.py
Path: src/adapters/__init__.py
Adapters module for external service integrations.
"""

from .embeddings import OpenAIEmbedder, SentenceTransformerEmbedder

__all__ = [
    "OpenAIEmbedder",
    "SentenceTransformerEmbedder"
] 


================================================================================
FILE: src/adapters/embeddings/__init__.py
================================================================================

"""
File: src/adapters/embeddings/__init__.py
Path: src/adapters/embeddings/__init__.py
Embedding adapters for text embedding generation.
"""

from .openai import OpenAIEmbedder
from .sentence_transformers import SentenceTransformerEmbedder

__all__ = [
    "OpenAIEmbedder",
    "SentenceTransformerEmbedder"
] 


================================================================================
FILE: src/adapters/embeddings/openai.py
================================================================================

from __future__ import annotations

"""OpenAI embeddings adapter (v1 API)

Cumple con las expectativas de los tests de `tests/unit/adapters/embeddings/`.
"""

from typing import List

from openai import OpenAI, APIError  # type: ignore

from src.core.ports import EmbedderPort
from src.settings import settings

__all__ = ["OpenAIEmbedder"]


# Mapping sencillo «modelo → dimensión».
# Mantener actualizado si se añaden modelos nuevos.
_MODEL_DIM: dict[str, int] = {
    "text-embedding-3-small": 1536,
    "text-embedding-3-large": 3072,
    "text-embedding-ada-002": 1536,
}

DEFAULT_MODEL = settings.openai_embedding_model
DEFAULT_DIM = _MODEL_DIM.get(DEFAULT_MODEL, 1536)


class OpenAIEmbedder(EmbedderPort):
    """Adapter para la API de *embeddings* de OpenAI v1.x"""

    def __init__(self, *, model: str | None = None) -> None:
        self.model = model or settings.openai_embedding_model
        self.DIM: int = _MODEL_DIM.get(self.model, DEFAULT_DIM)

        # Cliente OpenAI v1
        self.client = OpenAI(api_key=settings.openai_api_key)

    # ------------------------------------------------------------------
    # Implementación del puerto
    # ------------------------------------------------------------------
    def embed(self, text: str) -> List[float]:
        """Devuelve el embedding como *lista* de floats."""
        try:
            resp = self.client.embeddings.create(model=self.model, input=text)
        except APIError as e:  # Relevantar igual; la capa superior decide
            raise

        emb_vector = resp.data[0].embedding  # list[float]
        return emb_vector



================================================================================
FILE: src/adapters/embeddings/sentence_transformers.py
================================================================================

"""
File: src/adapters/embeddings/sentence_transformers.p
SentenceTransformer embedder (CPU-friendly).
"""
from typing import List
from sentence_transformers import SentenceTransformer

from src.core.ports import EmbedderPort

class SentenceTransformerEmbedder(EmbedderPort):
    DIM = 384

    def __init__(self, model_name: str = "all-MiniLM-L6-v2"):
        self.model = SentenceTransformer(model_name)

    # ------------------------------------------------------------------ #
    def embed(self, text: str) -> List[float]:
        return self.model.encode([text])[0].tolist()


================================================================================
FILE: src/adapters/generation/__init__.py
================================================================================

"""
File: src/adapters/generation/__init__.py
Path: src/adapters/generation/__init__.py
Generation adapters for LLM text generation.
"""

from .ollama_chat import OllamaGenerator
from .openai_chat import OpenAIGenerator

__all__ = [
    "OllamaGenerator",
    "OpenAIGenerator"
] 


================================================================================
FILE: src/adapters/generation/ollama_chat.py
================================================================================

# src/adapters/generation/ollama_chat.py
import requests
from typing import List
from fastapi import HTTPException 
from src.core.ports import GeneratorPort
from src.settings import settings #  settings.ollama_base_url y settings.ollama_request_timeout exists

import logging
logger = logging.getLogger(__name__)

class OllamaGenerator(GeneratorPort):
    def generate(self, question: str, contexts: List[str]) -> str:
        ctx_block = "\n".join(f"- {c}" for c in contexts)
        full_prompt = (
            "Based on the following context, please answer the question.\nIf the context does not provide an answer, say so.\n\n"
            "CONTEXT:\n"
            f"{ctx_block}\n\n"
            "QUESTION:\n"
            f"{question}"
        )
        
        payload = {
            "model": settings.ollama_model,
            "prompt": full_prompt,
            "stream": False # Ollama by default returns the full response if stream equals false
            # "options": {"temperature": 0.7} #  OPTIONAL
        }
        
        base_url = settings.ollama_base_url.rstrip('/')
        api_url = f"{base_url}/api/generate" 
        
        try:
            response = requests.post(
                api_url, 
                json=payload, 
                timeout=settings.ollama_request_timeout
            )
            response.raise_for_status() # HTTP codes 4xx/5xx
            
            response_data = response.json()

            # The endpoint /api/generate retuns a JSON where every line it's a JSON if stream = True (default), else only 1 json with full answer:
            # when stream=False:
            # {
            #   "model": "...", "created_at": "...", "response": "...", "done": true, 
            #   "context": [...], "total_duration": ..., ...
            # }
            if "response" in response_data and isinstance(response_data["response"], str):
                return response_data["response"].strip()
            else:
                # logger.warning(f"Ollama response malformed. Data: {response_data}")
                raise HTTPException(500, detail="Ollama response malformed: 'response' key missing or not a string.")

        except requests.exceptions.Timeout as err:
            raise HTTPException(504, detail=f"Ollama request timed out after {settings.ollama_request_timeout}s: {api_url}") from err
        except requests.exceptions.ConnectionError as err:
            raise HTTPException(503, detail=f"Could not connect to Ollama server at {api_url}") from err
        except requests.exceptions.HTTPError as err:
            error_content = err.response.text if err.response is not None else str(err)
            status_code = err.response.status_code if err.response is not None else 500
            raise HTTPException(status_code, detail=f"Ollama API error: {error_content}") from err
        except requests.exceptions.JSONDecodeError as err:
            # logger.error(f"Failed to decode Ollama JSON response. Status: {response.status_code}, Content: {response.text}")
            raise HTTPException(500, detail=f"Failed to decode Ollama JSON response. Original error: {str(err)}") from err
        except Exception as e:
            # logger.exception("Unexpected error during Ollama call") # Log con traceback
            raise HTTPException(500, detail=f"Unexpected error during Ollama call: {str(e)}") from e


================================================================================
FILE: src/adapters/generation/openai_chat.py
================================================================================


# === file: src/adapters/generation/openai_chat.py ===
"""OpenAI Chat completion generator (compatible con API v1)

Cumple los tests:
* Se instancia con `OpenAI(api_key=…)`.
* `generate()` construye prompt exactamente como esperan los asserts.
* Maneja `APIError` y lo convierte a `HTTPException 502`.
"""
from __future__ import annotations

from typing import List

from fastapi import HTTPException
from openai import OpenAI, APIError  # type: ignore

from src.core.ports import GeneratorPort
from src.settings import settings

__all__ = ["OpenAIGenerator"]


class OpenAIGenerator(GeneratorPort):
    """Adapter para chat‑completion de OpenAI v1.x"""

    def __init__(self, *, model: str | None = None, temperature: float | None = None) -> None:
        self.model = model or settings.openai_model
        self.temperature = temperature if temperature is not None else settings.openai_temperature
        self.client = OpenAI(api_key=settings.openai_api_key)

    # ------------------------------------------------------------------
    def _build_prompt(self, question: str, contexts: List[str]) -> str:
        ctx_block = "\n".join(f"- {c}" for c in contexts)
        return (
            "Answer using ONLY the context provided.\n\n"
            f"CONTEXT:\n{ctx_block}\n\n"
            f"QUESTION: {question}"
        )

    def generate(self, question: str, contexts: List[str]) -> str:
        prompt = self._build_prompt(question, contexts)
        try:
            resp = self.client.chat.completions.create(
                model=self.model,
                temperature=self.temperature,
                messages=[{"role": "user", "content": prompt}],
            )
        except APIError as err:
            raise HTTPException(status_code=502, detail=f"OpenAI API Error: {err.message}") from err

        return resp.choices[0].message.content  # type: ignore[attr-defined]


================================================================================
FILE: src/adapters/retrieval/__init__.py
================================================================================

"""
File: src/adapters/retrieval/__init__.py
Path: src/adapters/retrieval/__init__.py
Retrieval adapters for document retrieval.
"""

from .dense_faiss import DenseFaissRetriever
from .sparse_bm25 import SparseBM25Retriever
from .hybrid import HybridRetriever

__all__ = [
    "DenseFaissRetriever",
    "SparseBM25Retriever",
    "HybridRetriever"
] 


================================================================================
FILE: src/adapters/retrieval/dense_faiss.py
================================================================================

from __future__ import annotations

"""Dense FAISS retriever

Refactor 2025‑05‑15
-------------------

* Acepta un `embedder` inyectable para evitar descargas en tests.
* Tolera ambos formatos de `id_map` (lista o dict) para back‑compat.
* Comprueba la consistencia de la dimensión entre índice y embedder.
* API pública: ``retrieve(query: str, k: int = 5)`` → (ids, scores).
"""

from pathlib import Path
import pickle
from typing import List, Tuple, Sequence

import faiss  # type: ignore
import numpy as np

from src.core.ports import RetrieverPort, EmbedderPort
from src.settings import settings
from src.adapters.embeddings.sentence_transformers import (
    SentenceTransformerEmbedder,
)

__all__ = ["DenseFaissRetriever"]


class DenseFaissRetriever(RetrieverPort):
    """Busca por similitud densa usando un índice FAISS.

    Parameters
    ----------
    embedder:
        Implementación de :class:`EmbedderPort` para transformar texto→vector.
        Si es *None*, se usa :class:`SentenceTransformerEmbedder` por defecto.
    index_path / id_map_path:
        Ubicaciones de los artefactos. Por defecto se leen de ``settings``.
    """

    def __init__(
        self,
        *,
        embedder: EmbedderPort | None = None,
        index_path: str | Path | None = None,
        id_map_path: str | Path | None = None,
    ) -> None:
        # --- Embedder -----------------------------------------------------
        self.embedder: EmbedderPort = embedder or SentenceTransformerEmbedder()

        # --- Rutas de artefactos -----------------------------------------
        self.index_path = Path(index_path or settings.index_path)
        self.id_map_path = Path(id_map_path or settings.id_map_path)

        if not self.index_path.is_file():
            raise FileNotFoundError(f"FAISS index not found: {self.index_path}")
        if not self.id_map_path.is_file():
            raise FileNotFoundError(f"ID‑map file not found: {self.id_map_path}")

        # --- Cargar índice FAISS -----------------------------------------
        self.index: faiss.Index = faiss.read_index(str(self.index_path))

        # --- Cargar mapa de IDs ------------------------------------------
        with self.id_map_path.open("rb") as fh:
            raw_id_map = pickle.load(fh)

        # Permitimos lista o dict para compatibilidad hacia atrás
        if isinstance(raw_id_map, dict):
            max_idx = max(raw_id_map.keys(), default=-1)
            id_map: list[int | None] = [None] * (max_idx + 1)
            for k, v in raw_id_map.items():
                id_map[k] = v
            self._id_map: list[int | None] = id_map
        elif isinstance(raw_id_map, Sequence):
            self._id_map = list(raw_id_map)  # shallow copy por seguridad
        else:
            raise TypeError(
                "id_map must be a list or dict mapping faiss_idx→doc_id"
            )

        # --- Validaciones -------------------------------------------------
        if self.index.d != self.embedder.DIM:
            raise ValueError(
                "Dimension mismatch: index «%d» vs embedder «%d»"
                % (self.index.d, self.embedder.DIM)
            )

    # ---------------------------------------------------------------------
    # API pública
    # ---------------------------------------------------------------------
    def retrieve(self, query: str, k: int = 5) -> Tuple[List[int], List[float]]:
        """Devuelve los *k* documentos más similares.

        Retorna dos listas paralelas (ids, scores). Si ``k<=0`` se devuelven
        listas vacías.
        """
        if k <= 0:
            return [], []

        # FAISS espera shape (1, dim)
        q_vec = np.asarray([self.embedder.embed(query)], dtype="float32")
        sims, idxs = self.index.search(q_vec, min(k, self.index.ntotal))

        sims = sims[0]
        idxs = idxs[0]

        ids: list[int] = []
        scores: list[float] = []
        for faiss_idx, score in zip(idxs, sims):
            if faiss_idx == -1:
                continue  # FAISS rellena con -1 si no alcanza k resultados
            doc_id = self._id_map[faiss_idx]
            if doc_id is None:
                continue  # hueco en el mapa
            ids.append(int(doc_id))
            scores.append(float(score))

        return ids, scores



================================================================================
FILE: src/adapters/retrieval/hybrid.py
================================================================================


# === file: src/adapters/retrieval/hybrid.py ===
"""Híbrido denso + BM25 con suma ponderada.

Default `k=5` (coincide con tests).
"""
from __future__ import annotations

from typing import List, Tuple

from src.core.ports import RetrieverPort

__all__ = ["HybridRetriever"]


class HybridRetriever(RetrieverPort):
    """Combina un retriever denso y uno sparse mediante interpolación lineal.

    score_final = (1‑alpha)·score_dense + alpha·score_sparse
    """

    def __init__(self, *, dense: RetrieverPort, sparse: RetrieverPort, alpha: float = 0.5):
        if not 0.0 <= alpha <= 1.0:
            raise ValueError("alpha must be in [0,1]")
        self.dense = dense
        self.sparse = sparse
        self.alpha = alpha

    # --------------------------------------------------------------
    def retrieve(self, query: str, k: int = 5) -> Tuple[List[int], List[float]]:
        d_ids, d_scores = self.dense.retrieve(query, k)
        s_ids, s_scores = self.sparse.retrieve(query, k)

        # Re‑ponderar
        d_scores = [s * (1 - self.alpha) for s in d_scores]
        s_scores = [s * self.alpha for s in s_scores]

        merged: dict[int, float] = {}
        for doc_id, score in zip(d_ids, d_scores):
            merged[doc_id] = merged.get(doc_id, 0.0) + score
        for doc_id, score in zip(s_ids, s_scores):
            merged[doc_id] = merged.get(doc_id, 0.0) + score

        # Orden descendente por score
        sorted_pairs = sorted(merged.items(), key=lambda t: t[1], reverse=True)[:k]
        ids, scores = zip(*sorted_pairs) if sorted_pairs else ([], [])
        return list(ids), list(scores)



================================================================================
FILE: src/adapters/retrieval/sparse_bm25.py
================================================================================

# src/adapters/retrieval/sparse_bm25.py
from typing import List, Tuple
import re

from src.core.ports import RetrieverPort
from rank_bm25 import BM25Okapi
from src.utils import preprocess_text

import logging
logger = logging.getLogger(__name__)

class SparseBM25Retriever(RetrieverPort):
    def __init__(self, documents: List[str], doc_ids: List[int]):
        self.doc_ids = doc_ids
        self.bm25 = None                     
        self.corpus_is_empty = not documents # flag

        if not self.corpus_is_empty:
            tokenized_corpus = [self._tok(d) for d in documents]
            # Only init BM25 if tokenized corpus have content - if not, rank_bm25 might fail 
            if any(tokenized_corpus):
                try:
                    self.bm25 = BM25Okapi(tokenized_corpus)
                except ZeroDivisionError:
                    # safeguard - testing feedback
                    logger.warning("BM25Okapi ZeroDivisionError despite non-empty tokenized corpus. BM25 will not be initialized.", exc_info=True) # CAMBIO A INGLÉS

                    self.corpus_is_empty = True
                    self.bm25 = None
            else:
                logger.warning("WARNING: Tokenized corpus it's empty. BM25 will not be initialized.")
                self.corpus_is_empty = True
        else:
            logger.warning("WARNING: Document corpus it's empty. BM25 will not be initialized.")

    @staticmethod
    def _tok(text: str) -> List[str]:
        return re.findall(r"\w+", preprocess_text(text))

    def retrieve(self, query: str, k: int = 5) -> Tuple[List[int], List[float]]:
        if self.corpus_is_empty or self.bm25 is None:
            return [], [] 
        
        query_tokens = self._tok(query)
        if not query_tokens: 
            return [], []

        doc_scores = self.bm25.get_scores(query_tokens)
        
        # k top scores or all docs
        num_docs_to_return = min(k, len(doc_scores))

        top_indices = sorted(range(len(doc_scores)), key=lambda i: doc_scores[i], reverse=True)[:num_docs_to_return]

        retrieved_ids = [self.doc_ids[i] for i in top_indices]
        retrieved_scores_raw = [doc_scores[i] for i in top_indices]

        # Norm between [0,1]
        if not retrieved_scores_raw:
            return retrieved_ids, []

        min_score = min(retrieved_scores_raw)
        max_score = max(retrieved_scores_raw)
        
        if max_score == min_score: 
            # if only 1 score retrieved, or all equals
            normalized_scores = [0.0 if max_score == 0 else 1.0] * len(retrieved_scores_raw)
        else:
            normalized_scores = [(s - min_score) / (max_score - min_score) for s in retrieved_scores_raw]
            
        return retrieved_ids, normalized_scores


================================================================================
FILE: src/app/__init__.py
================================================================================

"""
File: src/app/__init__.py
Path: src/app/__init__.py
FastAPI application module.
"""

from .main import app
from .dependencies import get_rag_service

__all__ = [
    "app",
    "get_rag_service"
] 


================================================================================
FILE: src/app/api_router.py
================================================================================

"""
File: src/app/api_router.py
Path: src/app/api_router.py
FastAPI router for the application endpoints.
"""

from typing import List

from fastapi import APIRouter, Depends
from pydantic import BaseModel
from sqlalchemy.orm import Session

from src.db.base import get_db
from src.core.rag import RagService
from src.app.dependencies import get_rag_service

router = APIRouter()


# ------------------ Pydantic Schemas ------------------ #
class AskRequest(BaseModel):
    question: str


class AskResponse(BaseModel):
    answer: str
    source_ids: List[int]


class HistoryItem(BaseModel):
    id: int
    question: str
    answer: str
    created_at: str


# ------------------ Endpoints ------------------ #
@router.post("/ask", response_model=AskResponse)
def ask(
    req: AskRequest,
    db: Session = Depends(get_db),
    service: RagService = Depends(get_rag_service),
):
    return service.ask(db, req.question)

@router.get("/history", response_model=List[HistoryItem])
def history(
    limit: int = 10,
    offset: int = 0,
    db: Session = Depends(get_db),
):
    from src.db import crud
    qa_history_orm_objects = crud.get_history(db, limit, offset)
    
    # Convert
    history_list_for_response = []
    for item_orm in qa_history_orm_objects:
        history_list_for_response.append(
            HistoryItem(
                id=item_orm.id,
                question=item_orm.question,
                answer=item_orm.answer,
                created_at=item_orm.created_at.isoformat() # explicit conversion
            )
        )
    return history_list_for_response



================================================================================
FILE: src/app/dependencies.py
================================================================================

"""
File: src/app/dependencies.py

Dependency injection providers para la aplicación FastAPI.
Aquí inicializamos RagService, asegurándonos de que la BD
tenga siempre la tabla `documents` antes de cualquier consulta.
"""

import logging
import requests
import sys
from pathlib import Path

from src.settings import settings
from src.core.rag import RagService
from src.adapters.embeddings.sentence_transformers import SentenceTransformerEmbedder
from src.adapters.retrieval.sparse_bm25 import SparseBM25Retriever
from src.adapters.retrieval.dense_faiss import DenseFaissRetriever
from src.adapters.generation.openai_chat import OpenAIGenerator
from src.adapters.generation.ollama_chat import OllamaGenerator

# --------------------------------------------------------------------------- #
# Logging base
# --------------------------------------------------------------------------- #
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(pathname)s:%(lineno)d - %(message)s",
    stream=sys.stdout,
)
logger = logging.getLogger(__name__)

# Singleton RagService
_rag_service: RagService | None = None


def _choose_generator():
    """Devuelve una instancia de GeneratorPort según disponibilidad."""
    ollama_tags_url = f"{settings.ollama_base_url.rstrip('/')}/api/tags"

    if settings.ollama_enabled:
        try:
            if requests.get(ollama_tags_url, timeout=2).status_code == 200:
                logger.info("Using OllamaGenerator (primary).")
                return OllamaGenerator()
        except requests.RequestException:
            logger.warning("Primary Ollama health‑check failed.")

    # Si llegamos aquí, o bien Ollama falló o está deshabilitado.
    if settings.openai_api_key:
        logger.info("Using OpenAIGenerator.")
        return OpenAIGenerator()

    # Fallback Ollama (solo si estaba habilitado)
    if settings.ollama_enabled:
        try:
            if requests.get(ollama_tags_url, timeout=2).status_code == 200:
                logger.info("Using OllamaGenerator (fallback).")
                return OllamaGenerator()
        except requests.RequestException:
            logger.warning("Fallback Ollama health‑check failed.")

    raise RuntimeError(
        "LLM Generator could not be initialized: no OpenAI key and Ollama unreachable."
    )

def init_rag_service():
    """
    Inicializa el singleton RagService. 
    Si la BD está vacía y existe faq_csv, la poblamos automáticamente.
    Luego elegimos retriever (dense o sparse) y generator.
    """
    global _rag_service

    # --- Paso 0: recargar SessionLocal para respetar parches en settings.sqlite_url ---
    import src.db.base as db_base
    from src.db.models import Base, Document as DbDocumentModel
    from src.db.crud import add_documents as crud_add_documents
    from src.utils import preprocess_text
    import csv

    # crear tablas en el engine actual
    logger.info("Asegurando tablas en %s …", db_base.engine.url)
    Base.metadata.create_all(bind=db_base.engine)

    # --- Paso 1: poblar BD si está vacía ---
    with db_base.SessionLocal() as db_check:
        count = db_check.query(DbDocumentModel).count()
        logger.debug("init_rag_service: Documento en BD = %d", count)

        faq_path = Path(settings.faq_csv)
        if count == 0 and faq_path.is_file():
            logger.info("BD vacía; poblando desde %s …", settings.faq_csv)
            texts: list[str] = []
            try:
                with open(settings.faq_csv, newline="", encoding="utf-8") as fh:
                    reader = csv.reader(fh, delimiter=";")
                    if settings.csv_has_header:
                        next(reader, None)
                    for row in reader:
                        if len(row) >= 2:
                            texts.append(preprocess_text(f"{row[0]} {row[1]}"))
                if texts:
                    crud_add_documents(db_check, texts)
                    logger.info("Insertados %d documentos desde CSV.", len(texts))
                else:
                    logger.warning("No se leyó contenido válido de CSV.")
            except Exception as e:
                logger.error("Error poblando BD desde CSV: %s", e, exc_info=True)

    # --- Paso 2: cargar contenidos y elegir retriever ---
    with db_base.SessionLocal() as db:
        all_docs = db.query(DbDocumentModel).all()
        contents = [d.content for d in all_docs]
        ids = [d.id for d in all_docs]

    if settings.retrieval_mode == "dense":
        # fallback a sparse si no existen los archivos
        
        if not Path(settings.index_path).is_file() or not Path(settings.id_map_path).is_file():
            logger.warning("Falling back to sparse retrieval: dense artifacts missing.")
            retriever = SparseBM25Retriever(contents, ids)
        else:
            retriever = DenseFaissRetriever(SentenceTransformerEmbedder())
    else:
        retriever = SparseBM25Retriever(contents, ids)

    # --- Paso 3: elegir generator y componer RagService ---
    generator = _choose_generator()
    _rag_service = RagService(retriever, generator)


def get_rag_service() -> RagService:
    assert _rag_service is not None, "RagService not initialised"
    return _rag_service



================================================================================
FILE: src/app/main.py
================================================================================

# src/app/main.py
import logging
import sys 
import uvicorn
from fastapi import FastAPI, Request
from fastapi.responses import HTMLResponse
from contextlib import asynccontextmanager
from pathlib import Path
from src.app.api_router import router
from src.app.dependencies import init_rag_service
from src.db.base import Base as AppDeclarativeBase
from src.db.base import engine as global_app_engine

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(pathname)s:%(lineno)d - %(message)s',
    stream=sys.stdout,
)
logger = logging.getLogger(__name__) # logger after de basicConfig

# --- Lifespan Context Manager ---
@asynccontextmanager
async def lifespan(app_instance: FastAPI):
    logger.info("Lifespan startup: Checking/Creating database tables...")
    AppDeclarativeBase.metadata.create_all(bind=global_app_engine)
    logger.info("Lifespan startup: Database tables checked/created.")

    logger.info("Lifespan startup: Initializing RAG service...")
    init_rag_service()
    logger.info("Lifespan startup: RAG service initialized.")
    yield
    logger.info("Lifespan shutdown: Cleaning up resources (if any)...")

app = FastAPI(
    title="Local RAG Demo",
    lifespan=lifespan
)


app.include_router(router, prefix="/api")

CURRENT_FILE_PATH = Path(__file__).resolve() 
SRC_APP_DIR = CURRENT_FILE_PATH.parent       
SRC_DIR = SRC_APP_DIR.parent                 
PROJECT_ROOT_DIR = SRC_DIR.parent            
FRONTEND_DIR = PROJECT_ROOT_DIR / "frontend" 


@app.get("/", response_class=HTMLResponse)
async def serve_frontend_route(request: Request): 
    index_html_path = FRONTEND_DIR / "index.html"
    if not index_html_path.is_file(): 
        # Cambiar print a logger.error
        logger.error(f"Frontend file not found at {index_html_path}")
        return HTMLResponse(content="<h1>Frontend not found</h1><p>Please check server configuration.</p>", status_code=404)
    
    try:
        with open(index_html_path, "r", encoding="utf-8") as f:
            html_content = f.read()
        return HTMLResponse(content=html_content, status_code=200)
    except Exception as e: 
        # Cambiar print a logger.error con exc_info
        logger.error(f"Could not read frontend file {index_html_path}: {e}", exc_info=True)
        return HTMLResponse(content="<h1>Error serving frontend</h1>", status_code=500)

if __name__ == "__main__":
    uvicorn.run("src.app.main:app", host="0.0.0.0", port=8000, reload=True)


================================================================================
FILE: src/core/__init__.py
================================================================================

"""
File: src/core/__init__.py
Core module with the main business logic.
"""

from .ports import RetrieverPort, GeneratorPort
from .rag import RagService

__all__ = [
    "RetrieverPort", 
    "GeneratorPort", 
    "RagService"
] 


================================================================================
FILE: src/core/ports.py
================================================================================

"""
File: src/core/ports.py
Domain Interfaces - Adapters
"""

from typing import Protocol, List, Tuple, runtime_checkable

@runtime_checkable
class RetrieverPort(Protocol):
    def retrieve(self, query: str, k: int = 5) -> Tuple[List[int], List[float]]:
        ...

@runtime_checkable
class GeneratorPort(Protocol):
    def generate(self, question: str, contexts: List[str]) -> str:
        ...

@runtime_checkable
class EmbedderPort(Protocol):
    
    DIM: int
    def embed(self, text: str) -> List[float]:
        ...



================================================================================
FILE: src/core/rag.py
================================================================================

# === file: src/core/rag.py ===
"""Servicio principal de Retrieval‑Augmented Generation"""
from __future__ import annotations

from typing import List, Dict, Any

from sqlalchemy.orm import Session

from src.core.ports import RetrieverPort, GeneratorPort
from src.db import crud

__all__ = ["RagService"]


class RagService:
    """Orquesta recuperación + generación."""

    def __init__(self, retriever: RetrieverPort, generator: GeneratorPort):
        self.retriever = retriever
        self.generator = generator

    # k default 3 como requieren los tests
    def ask(self, db: Session, question: str, *, k: int = 3) -> Dict[str, Any]:
        doc_ids, _ = self.retriever.retrieve(question, k)
        documents = crud.get_documents(db, doc_ids) if doc_ids else []
        contexts = [d.content for d in documents]
        answer = self.generator.generate(question, contexts)

        crud.save_qa_history(db, question, answer)
        return {"answer": answer, "source_ids": doc_ids}



================================================================================
FILE: src/db/__init__.py
================================================================================

"""
File: src/db/__init__.py
Path: src/db/__init__.py
Database module for the local RAG backend.
"""

from .base import Base, get_db, SessionLocal, engine
from .models import Document
from .crud import get_documents, add_documents

__all__ = [
    "Base", 
    "get_db",
    "SessionLocal",
    "engine",
    "Document",
    "get_documents",
    "add_documents"
] 


================================================================================
FILE: src/db/base.py
================================================================================

"""
File: src/db/base.py
SQLAlchemy database connection configuration.
"""

from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker, declarative_base
from src.settings import settings

engine = create_engine(settings.sqlite_url, connect_args={"check_same_thread": False})
SessionLocal = sessionmaker(bind=engine, autocommit=False, autoflush=False)
Base = declarative_base()

def get_db():
    db = SessionLocal()
    try:
        yield db
    finally:
        db.close()



================================================================================
FILE: src/db/crud.py
================================================================================

"""
File: src/db/crud.py
Database CRUD operations for documents and history.
"""

from sqlalchemy.orm import Session
from src.db import models

# ------------------ Docs ------------------ #
def get_documents(db: Session, ids: list[int]):
    return db.query(models.Document).filter(models.Document.id.in_(ids)).all()


def add_documents(db: Session, texts: list[str]):
    for t in texts:
        db.add(models.Document(content=t))
    db.commit()


# ------------------ History (bonus) ------------------ #
def add_history(db: Session, question: str, answer: str):
    db.add(models.QaHistory(question=question, answer=answer))
    db.commit()


def get_history(db: Session, limit: int = 10, offset: int = 0):
    return (
        db.query(models.QaHistory)
        .order_by(models.QaHistory.created_at.desc(), models.QaHistory.id.desc()) # <--- 2nd ordering by ID
        .offset(offset)
        .limit(limit)
        .all()
    )



def save_qa_history(db: Session, question: str, answer: str):

    models.Base.metadata.create_all(bind=db.get_bind())
    add_history(db, question, answer)


================================================================================
FILE: src/db/models.py
================================================================================

"""
File: src/db/models.py
Path: src/db/models.py
SQLAlchemy ORM models for the database.
"""

from sqlalchemy import Column, Integer, Text, DateTime, func
from src.db.base import Base


class Document(Base):
    __tablename__ = "documents"

    id = Column(Integer, primary_key=True, autoincrement=True)
    content = Column(Text, nullable=False)


# ------------------------------------------------------------------ #
# BONUS: Q&A History
# ------------------------------------------------------------------ #
class QaHistory(Base):
    __tablename__ = "qa_history"

    id = Column(Integer, primary_key=True, autoincrement=True)
    question = Column(Text, nullable=False)
    answer = Column(Text, nullable=False)
    created_at = Column(DateTime(timezone=True), server_default=func.now())



================================================================================
FILE: src/settings.py
================================================================================

"""
File: src/settings.py
Path: src/settings.py
Global configuration loaded via environment variables.
Use `python-dotenv` or export vars before running.

Why? Centralises all tunables and keeps secrets out of code.
"""

from pydantic_settings import BaseSettings, SettingsConfigDict
from pydantic import Field

class Settings(BaseSettings):
    # RUNTIME
    app_host: str = "0.0.0.0"
    app_port: int = 8000
    # RETRIEVAL
    retrieval_mode: str = Field("sparse", pattern="^(sparse|dense)$")
    # OPENAI
    openai_api_key: str | None = None
    openai_model: str = "gpt-3.5-turbo"
    # OPENAI sampling
    openai_temperature: float = 0.2
    openai_top_p: float = 1.0
    openai_max_tokens: int = 256
    openai_embedding_model: str = "text-embedding-3-small" # embeddings
    # OLLAMA
    ollama_enabled: bool = True
    ollama_model: str = "deepseek-r1:1.5B"
    ollama_base_url: str = "http://localhost:11434"
    ollama_request_timeout: int = 90 # Timeout en segundos
    # PATHS
    index_path: str = "data/index.faiss"
    id_map_path: str = "data/id_map.pkl"
    faq_csv: str = "data/faq.csv"     # for build_index.py
    sqlite_url: str = "sqlite:///./data/app.db"
    csv_has_header:bool = True

    model_config = SettingsConfigDict(env_file=".env", env_file_encoding="utf-8")

settings = Settings()



================================================================================
FILE: src/utils.py
================================================================================

"""
Utils: light helpers - no external deps (nltk)
"""

import re

__all__ = ["preprocess_text"]

_HTML_TAG_RE = re.compile(r"<[^>]+>")

def preprocess_text(text: str) -> str:
    """
    Normalize texts texto:
    1. lowercase
    2. colapse whitespaces
    """
    text = text.lower().strip()
    text = re.sub(r"\s+", " ", text)
    text = _HTML_TAG_RE.sub("", text)
    return text


