
================================================================================
FILE: tests/conftest.py
================================================================================

# tests/conftest.py
import sys
from pathlib import Path
import pytest
from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker, Session as SQLAlchemySession
from sqlalchemy.pool import StaticPool
import importlib
import logging

# --- 1. Add src to PYTHONPATH ---
PROJECT_ROOT = Path(__file__).resolve().parents[1]
SRC_PATH = PROJECT_ROOT / "src"
if SRC_PATH.is_dir() and str(PROJECT_ROOT) not in sys.path:
    sys.path.insert(0, str(PROJECT_ROOT))

logger = logging.getLogger(__name__)
logging.basicConfig(level=logging.INFO) # Basic config for test logs

# --- 2. Parchear Settings Globalmente para la Sesión de Test ---
# Import settings first, then patch.
from src.settings import settings as global_app_settings

# Use a named in-memory DB for consistency across the test session
# The 'file:test_rag_db?mode=memory&cache=shared' part is the name.
# 'uri=true' is important for these parameters.
NAMED_IN_MEMORY_DB_URL = "sqlite:///file:test_rag_db?mode=memory&cache=shared&uri=true"

global_app_settings.sqlite_url = NAMED_IN_MEMORY_DB_URL
global_app_settings.retrieval_mode = "sparse"  # Default for most tests
global_app_settings.openai_api_key = "sk-dummy-conftest-key"
global_app_settings.ollama_enabled = False
global_app_settings.faq_csv = str(PROJECT_ROOT / "tests/data/empty_faq_for_tests.csv") # Ensure this file exists or is handled
global_app_settings.csv_has_header = True # Assuming a default

# Create the dummy CSV if it doesn't exist, or ensure your tests handle its absence
empty_faq_path = Path(global_app_settings.faq_csv)
empty_faq_path.parent.mkdir(parents=True, exist_ok=True)
if not empty_faq_path.exists():
    with open(empty_faq_path, 'w', newline='', encoding='utf-8') as f:
        # if csv_has_header is True, write a header
        if global_app_settings.csv_has_header:
            import csv
            writer = csv.writer(f, delimiter=';')
            writer.writerow(["question", "answer"])
    logger.info(f"Created dummy FAQ CSV for tests at {empty_faq_path}")


# --- 3. Reconfigurar el Engine y SessionLocal de src.db.base ---
# Import modules AFTER settings are patched
import src.db.base as db_base_module
import src.db.models as db_models_module
from src.app import dependencies as app_dependencies_module # for resetting singleton

# Forzar recarga de db_base_module para asegurar que recoge los settings parcheados
# ANTES de que su engine sea creado a nivel de módulo si lo hace.
# Si db_base.engine es None o no se inicializa hasta que se llama a una función,
# el reload aquí puede ser menos critical, pero no daña.
importlib.reload(db_base_module)
# db_models_module reload is usually not needed unless its Base definition depends on engine at import time.

_test_engine = create_engine(
    global_app_settings.sqlite_url,  # This will be the NAMED_IN_MEMORY_DB_URL
    connect_args={"check_same_thread": False},
    poolclass=StaticPool, # StaticPool is crucial for shared in-memory DBs
    echo=False # Set to True for SQL debugging
)
logger.info(f"CONTEST: Created _test_engine (id: {id(_test_engine)}) with URL: {_test_engine.url}")

# Directly assign the test engine and reconfigure SessionLocal
# This is the single source of truth for the DB during tests.
db_base_module.engine = _test_engine
db_base_module.SessionLocal.configure(bind=_test_engine)
logger.info(f"CONTEST: Patched db_base_module.engine (id: {id(db_base_module.engine)})")
logger.info(f"CONTEST: Reconfigured db_base_module.SessionLocal to bind to _test_engine")


# --- 4. Fixture para crear el esquema de BD una vez por sesión ---
@pytest.fixture(scope="session", autouse=True)
def create_db_tables_session_scoped():
    """Crea todas las tablas de la BD una vez por sesión de test, usando el _test_engine."""
    logger.info(f"CONTEST (session scope): Creating DB tables on engine {id(_test_engine)}...")
    db_models_module.Base.metadata.create_all(bind=_test_engine)
    yield
    logger.info(f"CONTEST (session scope): Dropping DB tables on engine {id(_test_engine)}...")
    db_models_module.Base.metadata.drop_all(bind=_test_engine)
    _test_engine.dispose() # Clean up engine resources
    logger.info(f"CONTEST (session scope): Disposed _test_engine.")

# --- 5. Fixture para una sesión de BD limpia (tablas vacías) por CADA TEST ---
@pytest.fixture(scope="function")
def db_session() -> SQLAlchemySession: # Use the actual Session type hint
    """
    Proporciona una sesión de SQLAlchemy y asegura que todas las tablas estén vacías
    antes de que el test se ejecute. La sesión usa el _test_engine configurado.
    """
    session = db_base_module.SessionLocal()
    engine_for_session = session.get_bind()
    logger.info(f"CONTEST (db_session fixture): New session (id: {id(session)}) using engine {id(engine_for_session)} ({engine_for_session.url})")

    # Limpiar tablas
    for table in reversed(db_models_module.Base.metadata.sorted_tables):
        session.execute(table.delete())
    session.commit()
    logger.info(f"CONTEST (db_session fixture): Tables cleared for session {id(session)}.")

    try:
        yield session
    finally:
        session.close()
        logger.info(f"CONTEST (db_session fixture): Closed session {id(session)}.")


# --- 6. Fixture para resetear el singleton de RAG Service ---
@pytest.fixture(scope="function", autouse=True)
def reset_rag_service_singleton():
    """Asegura que el _rag_service es None antes de cada test."""
    # Es importante que esto se ejecute ANTES de tests que podrían llamar a init_rag_service
    if hasattr(app_dependencies_module, '_rag_service'):
        app_dependencies_module._rag_service = None
        logger.info("CONTEST (reset_rag_service_singleton): _rag_service reset to None.")
    yield # El test se ejecuta aquí
    # Opcional: Limpiar después del test si es necesario, pero generalmente
    # el reset al inicio de la siguiente prueba es suficiente.
    # if hasattr(app_dependencies_module, '_rag_service'):
    #     app_dependencies_module._rag_service = None


# --- 7. Fixtures para datos poblados y servicio inicializado (para tests de integración) ---

@pytest.fixture(scope="function")
def AppSessionLocal() -> sessionmaker:
    """Proporciona la SessionLocal configurada para los tests."""
    return db_base_module.SessionLocal


@pytest.fixture(scope="function")
def populated_db_session(db_session: SQLAlchemySession): # Depende de la sesión limpia
    """Puebla la DB con datos de prueba comunes para integración."""
    from src.db.models import Document as DbDocument # Import local

    logger.info(f"CONTEST (populated_db_session): Populating DB for session {id(db_session)}.")
    test_docs_data = [
        {"id": 101, "content": "The refund policy states you can request a refund within 14 days."},
        {"id": 102, "content": "To contact support, please email support@example.com."},
        {"id": 103, "content": "Available features include semantic search and document processing."},
    ]
    for doc_data in test_docs_data:
        # Check if exists to avoid conflicts if called multiple times or if IDs are fixed
        if not db_session.get(DbDocument, doc_data["id"]):
             db_session.add(DbDocument(**doc_data))
    db_session.commit()
    count = db_session.query(DbDocument).count()
    logger.info(f"CONTEST (populated_db_session): DB populated with {count} documents for session {id(db_session)}.")
    return db_session


@pytest.fixture(scope="function")
def initialized_rag_service(populated_db_session, monkeypatch): # Depende de la DB poblada
    """
    Inicializa RagService para tests de integración.
    La DB ya está poblada por populated_db_session.
    El singleton _rag_service ya ha sido reseteado por reset_rag_service_singleton.
    """
    logger.info(f"CONTEST (initialized_rag_service): Initializing RAG service.")

    # Asegurar que el CSV de FAQ para esta inicialización específica es el de prueba (o no existe si así se desea)
    # Esto es importante si init_rag_service intenta poblar desde CSV.
    # Si populated_db_session ya pobló, la condición `db.query(DbDocument).count() == 0` será falsa.
    monkeypatch.setattr(global_app_settings, "faq_csv", str(PROJECT_ROOT / "tests/data/dummy_faq_for_init.csv"))
    dummy_init_faq_path = Path(global_app_settings.faq_csv)
    dummy_init_faq_path.parent.mkdir(parents=True, exist_ok=True)
    with open(dummy_init_faq_path, 'w', newline='', encoding='utf-8') as f:
        import csv
        writer = csv.writer(f, delimiter=';')
        writer.writerow(["question", "answer"]) # Header
        writer.writerow(["Q_init1", "A_init1"])
    logger.info(f"CONTEST (initialized_rag_service): Set faq_csv to {dummy_init_faq_path} for this init.")


    # Recargar el módulo de dependencias para que vea el _rag_service = None y los settings correctos
    # y para asegurar que init_rag_service opera en un estado limpio.
    # Es crucial si init_rag_service usa settings que podrían haber cambiado.
    importlib.reload(app_dependencies_module)

    app_dependencies_module.init_rag_service() # Esto usará la DB poblada y los settings actuales
    service = app_dependencies_module.get_rag_service()
    logger.info(f"CONTEST (initialized_rag_service): RAG service initialized (id: {id(service)}).")

    yield service

    # Limpieza del singleton después del test (aunque reset_rag_service_singleton lo hará al inicio del siguiente)
    app_dependencies_module._rag_service = None
    logger.info(f"CONTEST (initialized_rag_service): RAG service singleton reset post-test.")

# Alias para la fixture que usa el test de integración original, si es necesario
@pytest.fixture(scope="function")
def initialized_rag_service_for_integration(initialized_rag_service):
    return initialized_rag_service

# Alias para la fixture que usa el test de integración original, si es necesario
@pytest.fixture(scope="function")
def populated_db_for_integration(populated_db_session):
    return populated_db_session


================================================================================
FILE: tests/integration/test_api_ask.py
================================================================================

# tests/integration/test_api_ask.py
import pytest
from fastapi.testclient import TestClient
from unittest import mock
import random
import importlib # Para recargar módulos si es necesario
from src.settings import settings
from src.app.main import app # app se importa después de conftest.py
from src.app import dependencies as app_dependencies_module # Para re-init

# --- Fixture para el TestClient (ya no necesita ser module-scoped si la app es estable) ---
@pytest.fixture(scope="function")
def client(populated_db_for_integration) -> TestClient: # Depende de la DB poblada
    # populated_db_for_integration se ejecuta primero, asegura que la DB tiene datos.
    # reset_rag_service_singleton (si es autouse) también se habrá ejecutado.
    from src.app import dependencies as app_dependencies # Para resetear el singleton si es necesario
    
    # Asegurar que el servicio se reinicializa para ESTA instancia de TestClient,
    # especialmente si los settings fueron monkeypatcheados por un test anterior.
    app_dependencies._rag_service = None 
    importlib.reload(app_dependencies) # Recarga para asegurar que init_rag_service usa los settings actuales

    # Cuando TestClient(app) se crea, el lifespan de la app se ejecuta,
    # llamando a init_rag_service. Esta llamada ahora encontrará la DB poblada.
    with TestClient(app) as c:
        yield c

# --- Fixture para limpiar la tabla de historial ---
@pytest.fixture(scope="function")
def clean_history_table():
    from src.db.base import SessionLocal # Usar el SessionLocal ya reconfigurado por conftest
    from src.db.models import QaHistory
    with SessionLocal() as db:
        db.query(QaHistory).delete()
        db.commit()
    yield

# --- Helper para mock de OpenAI API v1.x ---
def _make_openai_v1_mock(answer_text: str):
    mock_completion = mock.MagicMock()
    mock_completion.choices = [mock.MagicMock()]
    mock_completion.choices[0].message = mock.MagicMock()
    mock_completion.choices[0].message.content = answer_text
    return mock_completion

# --- Tests ---
@mock.patch("src.adapters.generation.openai_chat.OpenAI") # Path donde OpenAI es instanciado
def test_api_ask_with_openai_retrieves_and_generates(
    MockOpenAIClass,
    client: TestClient,
    monkeypatch
):
    # Arrange
    # 1. Asegurar que se usará OpenAIGenerator y que tiene una API key para __init__
    monkeypatch.setattr(settings, "ollama_enabled", False)
    current_openai_api_key = settings.openai_api_key # Guardar por si se parchea
    if settings.openai_api_key is None or settings.openai_api_key == "sk-dummy-test-key": # Evitar sobreescribir una real
        monkeypatch.setattr(settings, "openai_api_key", "sk-integration-openai-ask")

    # 2. Configurar el mock para la clase OpenAI
    mock_openai_client_instance = MockOpenAIClass.return_value
    expected_answer = "Mocked AI answer about our refund policy."
    mock_openai_client_instance.chat.completions.create.return_value = _make_openai_v1_mock(expected_answer)

    # 3. Forzar la re-inicialización de RagService para que use el OpenAIGenerator
    #    que a su vez usará la clase OpenAI mockeada.
    #    Y para que el retriever se cree con el modo correcto si lo cambiamos.
    monkeypatch.setattr(settings, "retrieval_mode", "sparse") # Asegurar modo para aserción de source_id
    importlib.reload(app_dependencies_module) # Recargar para que vea los settings parcheados
    app_dependencies_module.init_rag_service() # Recrea _rag_service

    question_text = "What is the refund policy?"
    payload = {"question": question_text}

    # Act
    response = client.post("/api/ask", json=payload)

    # Assert
    assert response.status_code == 200
    body = response.json()
    assert body["answer"] == expected_answer
    assert isinstance(body["source_ids"], list)
    assert 101 in body["source_ids"] # Basado en datos de prueba en conftest.py

    mock_openai_client_instance.chat.completions.create.assert_called_once()
    args, kwargs = mock_openai_client_instance.chat.completions.create.call_args
    sent_prompt_content = kwargs["messages"][0]["content"]
    assert question_text in sent_prompt_content
    assert "refund policy states" in sent_prompt_content.lower() # Contexto del Doc ID 1

    # Restaurar API key si se cambió, aunque monkeypatch debería hacerlo por test
    monkeypatch.setattr(settings, "openai_api_key", current_openai_api_key)


@mock.patch("requests.post") # Path donde requests.post es llamado por OllamaGenerator
def test_api_ask_with_ollama_retrieves_and_generates(
    mock_requests_post,
    client: TestClient,
    monkeypatch
):
    # Arrange
    # 1. Habilitar Ollama y deshabilitar retrieval denso para simplificar
    monkeypatch.setattr(settings, "ollama_enabled", True)
    monkeypatch.setattr(settings, "retrieval_mode", "sparse") # O el modo que quieras probar

    # 2. Configurar mock para requests.post (Ollama)
    expected_answer = "Mocked Ollama answer regarding support."
    mock_ollama_response_obj = mock.MagicMock()
    mock_ollama_response_obj.json.return_value = {"response": expected_answer}
    mock_ollama_response_obj.status_code = 200
    mock_ollama_response_obj.raise_for_status.return_value = None
    mock_requests_post.return_value = mock_ollama_response_obj

    # 3. Forzar la re-inicialización de RagService para que use OllamaGenerator
    #    y el retriever correcto.
    importlib.reload(app_dependencies_module)
    app_dependencies_module.init_rag_service()

    question_text = "How to contact support?"
    payload = {"question": question_text}

    # Act
    response = client.post("/api/ask", json=payload)

    # Assert
    assert response.status_code == 200
    body = response.json()
    assert body["answer"] == expected_answer
    assert 102 in body["source_ids"]
    mock_requests_post.assert_called_once()
    args, kwargs = mock_requests_post.call_args
    sent_payload = kwargs["json"]
    assert question_text in sent_payload["prompt"]
    assert "contact support, please email" in sent_payload["prompt"].lower() # Contexto del Doc ID 2


def test_api_ask_validation_error_missing_question(client: TestClient):
    response = client.post("/api/ask", json={})
    assert response.status_code == 422
    detail = response.json().get("detail", [])
    assert any("question" in error.get("loc", []) for error in detail if isinstance(error, dict) and "loc" in error)


def test_api_ask_validation_error_wrong_type(client: TestClient):
    response = client.post("/api/ask", json={"question": 123})
    assert response.status_code == 422
    detail = response.json().get("detail", [])
    assert any("Input should be a valid string" in error.get("msg", "") for error in detail if isinstance(error, dict) and "msg" in error)


@mock.patch("src.adapters.generation.openai_chat.OpenAI")
def test_history_endpoint_records_and_retrieves_qa(
    MockOpenAIClass,
    client: TestClient,
    monkeypatch,
    clean_history_table # Usar fixture para limpiar tabla
):
    # Arrange
    # 1. Configurar para usar OpenAIGenerator con mock
    monkeypatch.setattr(settings, "ollama_enabled", False)
    current_openai_api_key = settings.openai_api_key
    if settings.openai_api_key is None or settings.openai_api_key == "sk-dummy-test-key":
        monkeypatch.setattr(settings, "openai_api_key", "sk-integration-hist-test")
    
    mock_openai_client_instance = MockOpenAIClass.return_value

    # Forzar re-init de RagService para asegurar que usa estos settings
    # (aunque conftest.py ya lo hace una vez, ollama_enabled podría haber cambiado)
    importlib.reload(app_dependencies_module)
    app_dependencies_module.init_rag_service()

    # Primera Q&A
    q_text1 = f"History Q1 {random.randint(1000, 9999)}"
    ans_text1 = "Hist Ans 1"
    mock_openai_client_instance.chat.completions.create.return_value = _make_openai_v1_mock(ans_text1)
    resp1 = client.post("/api/ask", json={"question": q_text1})
    assert resp1.status_code == 200

    # Segunda Q&A
    q_text2 = f"History Q2 {random.randint(1000, 9999)}"
    ans_text2 = "Hist Ans 2"
    # Importante: .create es un mock, si la misma instancia de mock_openai_client_instance se usa,
    # su return_value para .create necesita ser reconfigurado si la respuesta cambia.
    mock_openai_client_instance.chat.completions.create.return_value = _make_openai_v1_mock(ans_text2)
    resp2 = client.post("/api/ask", json={"question": q_text2})
    assert resp2.status_code == 200
    
    # Act
    resp_hist = client.get("/api/history?limit=5")
    assert resp_hist.status_code == 200
    history_data = resp_hist.json()

    # Assert
    assert len(history_data) == 2 # Exactamente 2 debido a clean_history_table

    assert history_data[0]["question"] == q_text2 # Más reciente primero
    assert history_data[0]["answer"] == ans_text2
    assert history_data[1]["question"] == q_text1
    assert history_data[1]["answer"] == ans_text1

    assert mock_openai_client_instance.chat.completions.create.call_count == 2

    monkeypatch.setattr(settings, "openai_api_key", current_openai_api_key)


================================================================================
FILE: tests/unit/adapters/embeddings/test_openai_embeddings.py
================================================================================

# tests/unit/adapters/embeddings/test_openai_embeddings.py
import pytest
from unittest import mock
from openai import APIError # Importar el error de la API v1.x

from src.adapters.embeddings.openai import OpenAIEmbedder
from src.settings import settings # Para verificar el modelo usado

# No necesitamos la fixture openai_embedder_instance si instanciamos dentro de cada test
# donde el mock de la clase OpenAI está activo.

@mock.patch("src.adapters.embeddings.openai.OpenAI") # Mockea la CLASE OpenAI
def test_openai_embedder_happy_path(MockOpenAI, monkeypatch):
    # Arrange
    if settings.openai_api_key is None: # Asegurar API key para el __init__ del embedder
        monkeypatch.setattr(settings, "openai_api_key", "sk-dummy-for-happy-path")

    text_to_embed = "This is a test sentence for happy path."
    # Crear un vector de la dimensión correcta (1536 para text-embedding-3-small por defecto en settings)
    # Usamos el DIM del embedder que se instancia para asegurar consistencia.
    # La instanciación de OpenAIEmbedder debe ocurrir después de cualquier monkeypatch de settings.openai_embedding_model
    # para que DIM se calcule correctamente. Aquí usamos el DIM por defecto.
    
    # Primero instanciamos el embedder para obtener su DIM configurado
    # (asumiendo que settings.openai_embedding_model no se cambia para este test)
    temp_embedder_for_dim = OpenAIEmbedder()
    expected_dim = temp_embedder_for_dim.DIM
    expected_embedding_vector = [i * 0.001 for i in range(expected_dim)]


    mock_client_instance = MockOpenAI.return_value # La instancia que OpenAI() devolvería

    # Configurar el mock para que devuelva una estructura similar a la de OpenAI Embeddings API v1.x
    mock_embedding_api_response = mock.MagicMock()
    mock_embedding_data_item = mock.MagicMock()
    mock_embedding_data_item.embedding = expected_embedding_vector
    mock_embedding_api_response.data = [mock_embedding_data_item]
    
    mock_client_instance.embeddings.create.return_value = mock_embedding_api_response

    # Instanciar el embedder que realmente probaremos, DESPUÉS de que el mock esté activo
    # y después de cualquier monkeypatch que afecte a su __init__.
    embedder_under_test = OpenAIEmbedder() # Su __init__ llamará a MockOpenAI()

    # Act
    embedding_result = None
    try:
        embedding_result = embedder_under_test.embed(text_to_embed)
        mock_client_instance.embeddings.create.assert_called_once_with(
            model=settings.openai_embedding_model,
            input=text_to_embed
        )
    except Exception as e:
        pytest.fail(f"Embed en happy path falló inesperadamente: {e}")

    # Assert
    assert embedding_result == expected_embedding_vector
    assert embedder_under_test.DIM == expected_dim # Verificar que el DIM es el esperado


@mock.patch("src.adapters.embeddings.openai.OpenAI")
def test_openai_embedder_handles_api_error(MockOpenAI, monkeypatch):
    # Arrange
    if settings.openai_api_key is None:
        monkeypatch.setattr(settings, "openai_api_key", "sk-dummy-for-api-error")

    text_to_embed = "Sentence that will cause API error."
    
    mock_client_instance = MockOpenAI.return_value
    mock_client_instance.embeddings.create.side_effect = APIError(
        message="Mock Embedding API Error from test", request=None, body=None
    )

    # Instanciar el embedder que probaremos
    embedder_under_test = OpenAIEmbedder()

    # Act & Assert
    with pytest.raises(APIError) as exc_info:
        embedder_under_test.embed(text_to_embed)
    
    assert "Mock Embedding API Error from test" in str(exc_info.value)
    mock_client_instance.embeddings.create.assert_called_once_with(
        model=settings.openai_embedding_model, # Verificar que se intentó llamar con los params correctos
        input=text_to_embed
    )

@mock.patch("src.adapters.embeddings.openai.OpenAI")
def test_openai_embedder_dim_updates_with_model_setting(MockOpenAI, monkeypatch):
    # Arrange
    original_embedding_model = settings.openai_embedding_model # Guardar para restaurar
    
    # Caso 1: text-embedding-3-large
    monkeypatch.setattr(settings, "openai_embedding_model", "text-embedding-3-large")
    if settings.openai_api_key is None:
        monkeypatch.setattr(settings, "openai_api_key", "sk-dummy-for-dim-test-large")
    
    # MockOpenAI se aplica aquí, así que la instancia de OpenAI() será un mock
    embedder_large = OpenAIEmbedder() 
    assert embedder_large.DIM == 3072, "DIM should be 3072 for text-embedding-3-large"
    # No necesitamos verificar la llamada a .create() aquí, solo el __init__ y DIM.
    # MockOpenAI().assert_called_once() # Verifica que el constructor de OpenAI (mockeado) fue llamado.
                                      # Esto es un poco más avanzado, puede que necesites
                                      # mock_constructor = MockOpenAI; mock_constructor.assert_called_once()
                                      # o verificar que mock_client_instance fue "creado"
    MockOpenAI.assert_called_with(api_key=settings.openai_api_key) # Verificar args de OpenAI()

    # Limpiar el mock de llamada para el siguiente caso (si MockOpenAI fuera persistente entre llamadas)
    # pero como se re-instancia el embedder, MockOpenAI() se llama de nuevo.
    MockOpenAI.reset_mock() # Resetea conteos de llamadas, etc.

    # Caso 2: text-embedding-ada-002 (también tiene DIM 1536, pero probamos el setting)
    monkeypatch.setattr(settings, "openai_embedding_model", "text-embedding-ada-002")
    if settings.openai_api_key is None: # Redundante si ya se seteó, pero no daña
        monkeypatch.setattr(settings, "openai_api_key", "sk-dummy-for-dim-test-ada")

    embedder_ada = OpenAIEmbedder()
    assert embedder_ada.DIM == 1536, "DIM should be 1536 for text-embedding-ada-002"
    MockOpenAI.assert_called_with(api_key=settings.openai_api_key)

    # Restaurar el setting original para no afectar otros tests si se ejecutan en la misma sesión
    # (aunque pytest debería aislar, es buena práctica con monkeypatch a nivel de settings globales)
    monkeypatch.setattr(settings, "openai_embedding_model", original_embedding_model)


================================================================================
FILE: tests/unit/adapters/embeddings/test_sentence_transformer_embedder.py
================================================================================

# tests/unit/adapters/embeddings/test_sentence_transformer_embedder.py
from unittest import mock
import numpy as np # Para comparar arrays si es necesario

from src.adapters.embeddings.sentence_transformers import SentenceTransformerEmbedder
# Asumimos que SentenceTransformerEmbedder implementa EmbedderPort
# from src.core.ports import EmbedderPort 

# No necesitamos una fixture para instanciar SentenceTransformerEmbedder
# porque su __init__ es el que carga el modelo, y eso es lo que queremos mockear
# o controlar.

# El path para mockear es donde 'SentenceTransformer' (la clase de la librería)
# es importada y usada por tu adaptador.
@mock.patch("src.adapters.embeddings.sentence_transformers.SentenceTransformer")
def test_sentence_transformer_embedder_happy_path(MockSentenceTransformer):
    # Arrange
    model_name_expected = "all-MiniLM-L6-v2" # El default en tu adaptador
    text_to_embed = "This is a test sentence for sentence-transformer."
    
    # Configurar el mock de la instancia de SentenceTransformer y su método encode
    mock_model_instance = MockSentenceTransformer.return_value # Lo que SentenceTransformer(model_name) devolvería
    
    # SentenceTransformer().encode() devuelve un numpy array.
    # La DIM de tu adaptador es 384 para 'all-MiniLM-L6-v2'.
    expected_embedding_vector_np = np.array([i * 0.01 for i in range(384)], dtype=np.float32)
    # Tu adaptador convierte esto a una lista de Python floats.
    expected_embedding_list = expected_embedding_vector_np.tolist()
    
    mock_model_instance.encode.return_value = [expected_embedding_vector_np] # encode espera una lista de textos y devuelve una lista de arrays

    # Act
    # Instanciar el embedder. Su __init__ llamará a SentenceTransformer(model_name_expected),
    # que ahora está mockeado por MockSentenceTransformer.
    embedder = SentenceTransformerEmbedder(model_name=model_name_expected)
    embedding_result = embedder.embed(text_to_embed)

    # Assert
    # 1. Verificar que SentenceTransformer fue instanciado con el nombre de modelo correcto
    MockSentenceTransformer.assert_called_once_with(model_name_expected)
    
    # 2. Verificar que el método encode del modelo mockeado fue llamado correctamente
    mock_model_instance.encode.assert_called_once_with([text_to_embed]) # encode toma una lista de strings
    
    # 3. Verificar que el embedding devuelto es el esperado
    assert embedding_result == expected_embedding_list
    assert isinstance(embedding_result, list)
    assert len(embedding_result) == 384 # Verificar dimensión
    assert all(isinstance(x, float) for x in embedding_result) # Verificar tipo de los elementos

    # 4. Verificar el atributo DIM del embedder
    assert embedder.DIM == 384


@mock.patch("src.adapters.embeddings.sentence_transformers.SentenceTransformer")
def test_sentence_transformer_embedder_custom_model_name(MockSentenceTransformer):
    # Arrange
    custom_model_name = "paraphrase-multilingual-MiniLM-L12-v2"
    text_to_embed = "Otra frase de prueba."
    
    mock_model_instance = MockSentenceTransformer.return_value
    # Asumimos que este modelo también tiene DIM 384 para simplificar el mock,
    # aunque en realidad podría ser diferente. El test se enfoca en que el nombre del modelo se pase.
    # Si la DIM fuera diferente, el embedder.DIM debería reflejarlo (requeriría lógica en __init__ para actualizar DIM)
    # o el test debería mockear la DIM de forma diferente.
    # Por ahora, mantenemos la DIM de 384.
    expected_embedding_vector_np = np.array([i * 0.02 for i in range(384)], dtype=np.float32)
    expected_embedding_list = expected_embedding_vector_np.tolist()
    mock_model_instance.encode.return_value = [expected_embedding_vector_np]

    # Act
    embedder = SentenceTransformerEmbedder(model_name=custom_model_name) # Usar nombre custom
    embedding_result = embedder.embed(text_to_embed)

    # Assert
    MockSentenceTransformer.assert_called_once_with(custom_model_name) # Verificar nombre custom
    mock_model_instance.encode.assert_called_once_with([text_to_embed])
    assert embedding_result == expected_embedding_list
    # Nota: El atributo embedder.DIM se hardcodea a 384 en tu clase actualmente.
    # Si quisieras que fuera dinámico según el modelo, el __init__ necesitaría obtenerlo del modelo.
    # Para el test actual, el DIM seguirá siendo 384.
    assert embedder.DIM == 384 


================================================================================
FILE: tests/unit/adapters/generation/test_generation_ollama.py
================================================================================

# tests/unit/test_generation_ollama.py
import pytest
from unittest import mock
import requests # Para los tipos de excepciones
from fastapi import HTTPException

from src.adapters.generation.ollama_chat import OllamaGenerator
from src.settings import settings # Para las configuraciones

@pytest.fixture
def ollama_generator() -> OllamaGenerator:
    # Si necesitas mockear settings para este test, usa monkeypatch
    return OllamaGenerator()

@mock.patch("requests.post") # Mockea requests.post
def test_ollama_generator_happy_path(mock_post, ollama_generator: OllamaGenerator):
    # Arrange
    question = "Ollama test question?"
    contexts = ["Ollama context 1.", "Ollama context 2."]
    expected_ollama_answer = "This is a mock Ollama answer."

    mock_response_object = mock.MagicMock()
    mock_response_object.json.return_value = {"response": expected_ollama_answer}
    mock_response_object.status_code = 200
    mock_response_object.raise_for_status.return_value = None # No levanta error
    mock_post.return_value = mock_response_object

    # Act
    answer = ollama_generator.generate(question, contexts)

    # Assert
    assert answer == expected_ollama_answer
    mock_post.assert_called_once()
    
    args, kwargs = mock_post.call_args
    # print(args)
    # print(kwargs)
    
    expected_api_url = f"{settings.ollama_base_url.rstrip('/')}/api/generate"
    assert args[0] == expected_api_url
    
    payload = kwargs["json"]
    assert payload["model"] == settings.ollama_model
    assert payload["stream"] is False
    
    ctx_block_expected = "\n".join(f"- {c}" for c in contexts)
    full_prompt_expected = (
        "Based on the following context, please answer the question.\nIf the context does not provide an answer, say so.\n\n"
        "CONTEXT:\n"
        f"{ctx_block_expected}\n\n"
        "QUESTION:\n"
        f"{question}"
    )
    assert payload["prompt"] == full_prompt_expected
    assert kwargs["timeout"] == settings.ollama_request_timeout


@mock.patch("requests.post")
def test_ollama_generator_handles_connection_error(mock_post, ollama_generator: OllamaGenerator):
    # Arrange
    mock_post.side_effect = requests.exceptions.ConnectionError("Failed to connect")

    # Act & Assert
    with pytest.raises(HTTPException) as exc_info:
        ollama_generator.generate("q", ["c"])
    
    assert exc_info.value.status_code == 503
    assert "Could not connect to Ollama server" in exc_info.value.detail


@mock.patch("requests.post")
def test_ollama_generator_handles_http_error(mock_post, ollama_generator: OllamaGenerator):
    # Arrange
    mock_response_object = mock.MagicMock()
    mock_response_object.status_code = 500
    mock_response_object.text = "Internal Server Error from Ollama"
    mock_response_object.raise_for_status.side_effect = requests.exceptions.HTTPError(
        "Mock HTTP Error", response=mock_response_object
    )
    mock_post.return_value = mock_response_object
    
    # Act & Assert
    with pytest.raises(HTTPException) as exc_info:
        ollama_generator.generate("q", ["c"])
        
    assert exc_info.value.status_code == 500
    assert "Ollama API error: Internal Server Error from Ollama" in exc_info.value.detail

@mock.patch("requests.post")
def test_ollama_generator_handles_malformed_json_response(mock_post, ollama_generator: OllamaGenerator):
    # Arrange
    mock_response_object = mock.MagicMock()
    mock_response_object.status_code = 200
    mock_response_object.raise_for_status.return_value = None
    mock_response_object.json.return_value = {"error": "unexpected_format_no_response_key"} # Falta la clave 'response'
    mock_post.return_value = mock_response_object

    # Act & Assert
    with pytest.raises(HTTPException) as exc_info:
        ollama_generator.generate("q", ["c"])
    
    assert exc_info.value.status_code == 500
    assert "Ollama response malformed: 'response' key missing" in exc_info.value.detail


================================================================================
FILE: tests/unit/adapters/generation/test_generation_openai.py
================================================================================

# tests/unit/test_generation_openai.py
import pytest
from unittest import mock
from fastapi import HTTPException
from openai import APIError # Importar el nuevo error

from src.adapters.generation.openai_chat import OpenAIGenerator
from src.settings import settings

@pytest.fixture
def openai_generator_instance(monkeypatch) -> OpenAIGenerator:
    # Si settings.openai_api_key es None y confías en la var de entorno
    monkeypatch.setenv("OPENAI_API_KEY", "sk-dummy-test-key-for-env")
    generator = OpenAIGenerator()
    monkeypatch.delenv("OPENAI_API_KEY", raising=False) # Limpiar después
    return generator

# El path para mockear cambia a donde se encuentra el método que queremos interceptar.
# Como 'self.client' se crea en el __init__ de OpenAIGenerator, y luego se llama
# 'self.client.chat.completions.create', necesitamos mockear ese método.
# Esto se puede hacer mockeando la clase OpenAI y su cadena de atributos.
@mock.patch("src.adapters.generation.openai_chat.OpenAI") # Mockea la clase OpenAI donde se importa
def test_openai_generator_happy_path(MockOpenAI, openai_generator_instance: OpenAIGenerator):
    # Arrange
    question = "Test question?"
    contexts = ["Context 1.", "Context 2 snippet."]
    expected_generated_answer = "This is a mock AI answer for API v1."

    # Configurar el mock de la instancia del cliente y su método
    mock_client_instance = MockOpenAI.return_value # Esto es lo que self.client será
    mock_completion = mock.MagicMock()
    mock_completion.choices = [mock.MagicMock()]
    mock_completion.choices[0].message = mock.MagicMock()
    mock_completion.choices[0].message.content = expected_generated_answer
    
    # El método mockeado ahora es en la instancia del cliente
    mock_client_instance.chat.completions.create.return_value = mock_completion

    # Act
    # Necesitamos re-instanciar OpenAIGenerator DESPUÉS de que MockOpenAI esté activo
    # para que su __init__ use el cliente mockeado, O inyectar el cliente mockeado.
    # La forma más simple es que la fixture ya use el mock si es posible, o
    # que el generador se cree aquí.
    
    # Si OpenAIGenerator crea self.client en __init__, y la fixture lo crea antes del mock,
    # el cliente no será el mockeado.
    # Opción 1: Recrear el generador aquí (más simple para este test)
    generator_under_test = OpenAIGenerator() # Su __init__ ahora usará MockOpenAI().return_value

    answer = generator_under_test.generate(question, contexts)

    # Assert
    assert answer == expected_generated_answer

    # Verificar que el método create fue llamado una vez en la instancia mockeada del cliente
    mock_client_instance.chat.completions.create.assert_called_once()
    
    args, kwargs = mock_client_instance.chat.completions.create.call_args
    # print(kwargs) # Descomenta para ver los kwargs

    assert kwargs["model"] == settings.openai_model
    # La API Key ya no se pasa a 'create', se usa al instanciar el cliente OpenAI.
    # assert kwargs["api_key"] == settings.openai_api_key # YA NO ES ASÍ
    assert kwargs["temperature"] == settings.openai_temperature

    messages = kwargs["messages"]
    assert len(messages) == 1
    assert messages[0]["role"] == "user"
    
    ctx_block_expected = "\n".join(f"- {c}" for c in contexts)
    prompt_content_expected = (
        "Answer using ONLY the context provided.\n\n"
        f"CONTEXT:\n{ctx_block_expected}\n\n"
        f"QUESTION: {question}"
    )
    assert messages[0]["content"] == prompt_content_expected


@mock.patch("src.adapters.generation.openai_chat.OpenAI")
def test_openai_generator_handles_api_error(MockOpenAI, openai_generator_instance: OpenAIGenerator): # La fixture no se usa directamente aquí
    # Arrange
    question = "Another question"
    contexts = ["Some context."]
    
    mock_client_instance = MockOpenAI.return_value
    # Configurar el mock para que levante un APIError de OpenAI v1.x
    # El error puede tener un 'response' mockeado si tu código lo usa.
    # Para un error simple, un mensaje es suficiente.
    # Necesitas crear un objeto mock de respuesta si tu manejo de errores lo espera.
    # Por ahora, un APIError simple.
    mock_client_instance.chat.completions.create.side_effect = APIError(
        message="Mock API Error from v1", 
        request=None, # puedes mockear request si es necesario
        body=None     # puedes mockear body si es necesario
    )
    # Si necesitas simular un status_code específico, APIError lo puede tomar o se puede
    # construir un mock_response para el error.
    # error_response = mock.MagicMock()
    # error_response.status_code = 429 # Ejemplo
    # mock_client_instance.chat.completions.create.side_effect = APIError(
    #     message="Rate limit exceeded", request=None, body=None, response=error_response
    # )


    generator_under_test = OpenAIGenerator() # Para que use el cliente mockeado

    # Act & Assert
    with pytest.raises(HTTPException) as exc_info:
        generator_under_test.generate(question, contexts)
    
    assert exc_info.value.status_code == 502 # O el código que decidas para el error
    # El mensaje de error ahora vendrá de err.message del APIError
    assert "OpenAI API Error: Mock API Error from v1" in str(exc_info.value.detail)

    mock_client_instance.chat.completions.create.assert_called_once()


================================================================================
FILE: tests/unit/adapters/retrieval/test_hybrid_retriever.py
================================================================================

# tests/unit/adapters/retrieval/test_hybrid_retriever.py
import pytest
from unittest.mock import Mock
from src.adapters.retrieval.hybrid import HybridRetriever
from src.core.ports import RetrieverPort # Asegúrate que este path sea correcto

@pytest.fixture
def mock_dense_retriever() -> Mock:
    retriever = Mock(spec=RetrieverPort)
    retriever.retrieve.return_value = ([], []) # Default: no devuelve nada
    return retriever

@pytest.fixture
def mock_sparse_retriever() -> Mock:
    retriever = Mock(spec=RetrieverPort)
    retriever.retrieve.return_value = ([], []) # Default: no devuelve nada
    return retriever

def test_hybrid_dense_only(mock_dense_retriever: Mock, mock_sparse_retriever: Mock):
    # Arrange
    dense_ids, dense_scores = [1, 2], [0.8, 0.7]
    mock_dense_retriever.retrieve.return_value = (dense_ids, dense_scores)
    mock_sparse_retriever.retrieve.return_value = ([], [])
    
    alpha = 0.5 # Weight for sparse, so (1-alpha) for dense
    hybrid_retriever = HybridRetriever(dense=mock_dense_retriever, sparse=mock_sparse_retriever, alpha=alpha)
    
    # Act
    retrieved_ids, retrieved_scores = hybrid_retriever.retrieve("test query")
    
    # Assert
    mock_dense_retriever.retrieve.assert_called_once_with("test query", 5) # k=5 es el default
    mock_sparse_retriever.retrieve.assert_called_once_with("test query", 5)

    assert retrieved_ids == [1, 2]
    # Scores should be (1-alpha) * original dense scores
    expected_scores = [s * (1 - alpha) for s in dense_scores]
    assert retrieved_scores == pytest.approx(expected_scores, rel=1e-9) # CORRECTO

def test_hybrid_sparse_only(mock_dense_retriever: Mock, mock_sparse_retriever: Mock):
    # Arrange
    sparse_ids, sparse_scores = [3, 4], [0.9, 0.6]
    mock_dense_retriever.retrieve.return_value = ([], [])
    mock_sparse_retriever.retrieve.return_value = (sparse_ids, sparse_scores)

    alpha = 0.5
    hybrid_retriever = HybridRetriever(dense=mock_dense_retriever, sparse=mock_sparse_retriever, alpha=alpha)

    # Act
    retrieved_ids, retrieved_scores = hybrid_retriever.retrieve("test query")

    # Assert
    assert retrieved_ids == [3, 4]
    # Scores should be alpha * original sparse scores
    expected_scores = [s * alpha for s in sparse_scores]
    assert pytest.approx(retrieved_scores, rel=1e-9) == expected_scores

def test_hybrid_both_no_overlap(mock_dense_retriever: Mock, mock_sparse_retriever: Mock):
    # Arrange
    mock_dense_retriever.retrieve.return_value = ([1], [0.8])
    mock_sparse_retriever.retrieve.return_value = ([2], [0.9])
    
    alpha = 0.5
    hybrid_retriever = HybridRetriever(dense=mock_dense_retriever, sparse=mock_sparse_retriever, alpha=alpha)

    # Act
    retrieved_ids, retrieved_scores = hybrid_retriever.retrieve("test query")

    # Assert
    # Dense score: 0.8 * (1-0.5) = 0.4
    # Sparse score: 0.9 * 0.5 = 0.45
    # Expected order: doc 2 (sparse) then doc 1 (dense)
    assert retrieved_ids == [2, 1]
    assert pytest.approx(retrieved_scores, rel=1e-9) == [0.45, 0.4]

def test_hybrid_both_with_overlap(mock_dense_retriever: Mock, mock_sparse_retriever: Mock):
    # Arrange
    mock_dense_retriever.retrieve.return_value = ([1, 2], [0.8, 0.6]) # doc 1, doc 2
    mock_sparse_retriever.retrieve.return_value = ([2, 3], [0.9, 0.7]) # doc 2, doc 3
    
    alpha = 0.5
    hybrid_retriever = HybridRetriever(dense=mock_dense_retriever, sparse=mock_sparse_retriever, alpha=alpha)

    # Act
    retrieved_ids, retrieved_scores = hybrid_retriever.retrieve("test query")

    # Assert
    # Doc 1 (dense only): 0.8 * (1-0.5) = 0.4
    # Doc 2 (both): (0.6 * (1-0.5)) + (0.9 * 0.5) = 0.3 + 0.45 = 0.75
    # Doc 3 (sparse only): 0.7 * 0.5 = 0.35
    # Expected order: Doc 2, Doc 1, Doc 3
    assert retrieved_ids == [2, 1, 3]
    assert pytest.approx(retrieved_scores, rel=1e-9) == [0.75, 0.4, 0.35]

@pytest.mark.parametrize("alpha_val, expected_id_order, expected_scores_approx", [
    (0.0, [10], [0.8]),   # Dense only
    (1.0, [10], [0.7]),   # Sparse only
    (0.3, [10], [0.8*0.7 + 0.7*0.3]), # Weighted, 0.56 + 0.21 = 0.77
    (0.7, [10], [0.8*0.3 + 0.7*0.7]), # Weighted, 0.24 + 0.49 = 0.73
])
def test_hybrid_alpha_variations(
    mock_dense_retriever: Mock,
    mock_sparse_retriever: Mock,
    alpha_val: float,
    expected_id_order: list[int],
    expected_scores_approx: list[float]
):
    # Arrange
    # Use a single common document to clearly see alpha's effect
    mock_dense_retriever.retrieve.return_value = ([10], [0.8])
    mock_sparse_retriever.retrieve.return_value = ([10], [0.7])
    
    hybrid_retriever = HybridRetriever(
        dense=mock_dense_retriever, 
        sparse=mock_sparse_retriever, 
        alpha=alpha_val
    )

    # Act
    retrieved_ids, retrieved_scores = hybrid_retriever.retrieve("test query")

    # Assert
    assert retrieved_ids == expected_id_order
    assert pytest.approx(retrieved_scores, rel=1e-9) == expected_scores_approx

def test_hybrid_respects_k_param(mock_dense_retriever: Mock, mock_sparse_retriever: Mock):
    # Arrange
    # Both retrievers return more docs than k
    mock_dense_retriever.retrieve.return_value = ([1, 2, 3], [0.9, 0.8, 0.7])
    mock_sparse_retriever.retrieve.return_value = ([3, 4, 5], [0.85, 0.75, 0.65])
    
    alpha = 0.5
    k_val = 2 # We want only top 2 results
    hybrid_retriever = HybridRetriever(dense=mock_dense_retriever, sparse=mock_sparse_retriever, alpha=alpha)

    # Act
    retrieved_ids, retrieved_scores = hybrid_retriever.retrieve("test query", k=k_val)

    # Assert
    mock_dense_retriever.retrieve.assert_called_once_with("test query", k_val)
    mock_sparse_retriever.retrieve.assert_called_once_with("test query", k_val)
    
    assert len(retrieved_ids) == k_val
    assert len(retrieved_scores) == k_val
    # Scores for context:
    # Doc 1 (dense): 0.9 * 0.5 = 0.45
    # Doc 2 (dense): 0.8 * 0.5 = 0.4
    # Doc 3 (both): (0.7 * 0.5) + (0.85 * 0.5) = 0.35 + 0.425 = 0.775
    # Doc 4 (sparse): 0.75 * 0.5 = 0.375
    # Doc 5 (sparse): 0.65 * 0.5 = 0.325
    # Top 2 expected: Doc 3, Doc 1
    assert retrieved_ids == [3, 1]
    assert pytest.approx(retrieved_scores, rel=1e-9) == [0.775, 0.45]

def test_hybrid_empty_results_from_both(mock_dense_retriever: Mock, mock_sparse_retriever: Mock):
    # Arrange
    mock_dense_retriever.retrieve.return_value = ([], [])
    mock_sparse_retriever.retrieve.return_value = ([], [])
    
    hybrid_retriever = HybridRetriever(dense=mock_dense_retriever, sparse=mock_sparse_retriever, alpha=0.5)

    # Act
    retrieved_ids, retrieved_scores = hybrid_retriever.retrieve("test query")

    # Assert
    assert retrieved_ids == []
    assert retrieved_scores == []


================================================================================
FILE: tests/unit/adapters/retrieval/test_retrieval_bm25.py
================================================================================

# tests/unit/test_retrieval_bm25.py
import pytest
from src.adapters.retrieval.sparse_bm25 import SparseBM25Retriever # o el path correcto

# --- Fixtures (si son necesarias) ---
@pytest.fixture
def sample_documents_data() -> tuple[list[str], list[int]]:
    documents = [
        "El perro marrón rápido salta sobre el zorro perezoso.",
        "Nunca la lluvia en España cae principalmente en la llanura.",
        "El perro es el mejor amigo del hombre.",
        "Los zorros son ágiles y los perros también.",
    ]
    doc_ids = [10, 20, 30, 40] # IDs arbitrarios para los documentos
    return documents, doc_ids

# --- Tests ---
def test_bm25_retrieves_relevant_doc(sample_documents_data):
    documents, doc_ids = sample_documents_data
    retriever = SparseBM25Retriever(documents=documents, doc_ids=doc_ids)
    
    query = "perro amigo"
    # Asumimos que `retrieve` ahora devuelve (ids, scores)
    retrieved_ids, retrieved_scores = retriever.retrieve(query, k=1)
    
    assert len(retrieved_ids) == 1
    assert retrieved_ids[0] == 30 # "El perro es el mejor amigo del hombre."
    assert len(retrieved_scores) == 1
    assert isinstance(retrieved_scores[0], float) # O el tipo que esperes

def test_bm25_respects_k(sample_documents_data):
    documents, doc_ids = sample_documents_data
    retriever = SparseBM25Retriever(documents=documents, doc_ids=doc_ids)
    
    query = "LES"
    k_val = 2
    retrieved_ids, retrieved_scores = retriever.retrieve(query, k=k_val)
    
    assert len(retrieved_ids) == k_val
    assert len(retrieved_scores) == k_val
    # Podrías hacer aserciones más específicas sobre los IDs esperados si el orden es predecible

def test_bm25_no_match(sample_documents_data):
    documents, doc_ids = sample_documents_data
    retriever = SparseBM25Retriever(documents=documents, doc_ids=doc_ids)
    
    query = "gato unicornio inexistente"
    retrieved_ids, retrieved_scores = retriever.retrieve(query, k=1)
    
    # BM25 aún podría devolver algo si hay solapamiento de tokens comunes
    # o podría devolver una lista vacía si los scores son muy bajos o cero.
    # Esto depende de la implementación de rank_bm25 y cómo manejas scores cero.
    # Si se esperan resultados (incluso con scores bajos):
    # assert len(retrieved_ids) > 0 # o un número específico
    # Si se espera una lista vacía para no coincidencias fuertes:
    # assert len(retrieved_ids) == 0
    # assert len(retrieved_scores) == 0
    # Por ahora, seamos flexibles y solo verifiquemos la estructura:
    assert isinstance(retrieved_ids, list)
    assert isinstance(retrieved_scores, list)
    assert len(retrieved_ids) == len(retrieved_scores)


def test_bm25_empty_query(sample_documents_data):
    documents, doc_ids = sample_documents_data
    retriever = SparseBM25Retriever(documents=documents, doc_ids=doc_ids)
    
    query = ""
    retrieved_ids, retrieved_scores = retriever.retrieve(query, k=1)

    # rank_bm25 con query vacía usualmente da scores de 0 para todos los docs.
    # Devolvería todos los documentos con score 0 si k es lo suficientemente grande,
    # o los primeros k documentos con score 0.
    assert len(retrieved_ids) <= 1 # Debería devolver k documentos o menos
    assert len(retrieved_scores) <= 1
    if retrieved_scores: # Si devuelve algo
        assert all(score == 0.0 for score in retrieved_scores)

# Podrías añadir más tests:
# - Con documentos vacíos
# - Verificando el orden de los scores (el primer score debe ser >= al segundo, etc.)


================================================================================
FILE: tests/unit/adapters/retrieval/test_retrieval_faiss.py
================================================================================

# tests/unit/test_retrieval_dense_faiss.py
import pytest
import numpy as np
import faiss
import pickle
from pathlib import Path

from src.settings import settings as global_settings # Para monkeypatch
from src.adapters.retrieval.dense_faiss import DenseFaissRetriever
from src.core.ports import EmbedderPort # Para nuestro DummyEmbedder

# --- Dummy Embedder ---
class DummyEmbedder(EmbedderPort):
    DIM: int # Debe coincidir con la dimensión del índice FAISS de prueba

    def __init__(self, dim: int, predefined_embeddings: dict[str, list[float]] = None):
        self.DIM = dim
        # predefined_embeddings: mapea texto de query/doc a su vector
        self.predefined_embeddings = predefined_embeddings if predefined_embeddings else {}
        # Default embedding si no se encuentra en predefined_embeddings
        self.default_vector = [0.0] * self.DIM

    def embed(self, text: str) -> list[float]:
        return self.predefined_embeddings.get(text, self.default_vector)

# --- Fixture para crear y limpiar artefactos de FAISS para el test ---
@pytest.fixture
def faiss_test_artefacts(tmp_path: Path) -> tuple[Path, Path, dict[str, list[float]], int]:
    dim = 4 # Dimensión pequeña para el test
    num_docs = 3

    # Documentos y sus embeddings predefinidos (simplificados)
    # Estos embeddings están diseñados para que la "query_target_doc1" esté más cerca de "doc1_text"
    doc_embeddings = {
        "doc1_text": [1.0, 0.1, 0.2, 0.3], # Target
        "doc2_text": [0.2, 1.0, 0.3, 0.4],
        "doc3_text": [0.3, 0.2, 1.0, 0.5],
    }
    doc_ids_in_db = [101, 102, 103] # IDs que estarían en la BD

    query_embeddings = {
        "query_target_doc1": [0.9, 0.15, 0.25, 0.35], # Muy similar a doc1_text
        "query_target_doc2": [0.25, 0.9, 0.35, 0.45], # Muy similar a doc2_text
        "query_no_match":    [0.0, 0.0, 0.0, 0.0],   # Diferente a todos
    }

    all_predefined_embeddings = {**doc_embeddings, **query_embeddings}

    # Crear el índice FAISS
    index_path = tmp_path / "test_index.faiss"
    id_map_path = tmp_path / "test_id_map.pkl"

    # Convertir doc_embeddings a numpy array para FAISS
    vectors_np = np.array(list(doc_embeddings.values()), dtype="float32")
    
    # Usar IndexFlatL2 para similaridad de coseno (después de normalizar) o distancia L2
    # Para este ejemplo, IndexFlatL2 (distancia Euclidiana) es más simple.
    # Si los embeddings estuvieran normalizados a longitud unitaria, L2 y producto interno son equivalentes.
    index = faiss.IndexFlatL2(dim)
    index.add(vectors_np)
    faiss.write_index(index, str(index_path))

    # Crear el id_map: mapea índice de FAISS (0, 1, 2) a ID de BD (101, 102, 103)
    id_map_content = {i: doc_id for i, doc_id in enumerate(doc_ids_in_db)}
    with open(id_map_path, "wb") as f:
        pickle.dump(id_map_content, f)

    return index_path, id_map_path, all_predefined_embeddings, dim


# --- Tests ---
def test_faiss_retrieves_closest_doc(faiss_test_artefacts, monkeypatch):
    index_path, id_map_path, predefined_embeddings, dim = faiss_test_artefacts

    # Monkeypatch settings para que el retriever use los artefactos de test
    monkeypatch.setattr(global_settings, "index_path", str(index_path))
    monkeypatch.setattr(global_settings, "id_map_path", str(id_map_path))

    embedder = DummyEmbedder(dim=dim, predefined_embeddings=predefined_embeddings)
    # Nota: DenseFaissRetriever toma una instancia de SentenceTransformerEmbedder en su __init__.
    # Para este test unitario, es mejor si DenseFaissRetriever aceptara un EmbedderPort.
    # Si no, tendremos que mockear SentenceTransformerEmbedder o crear una subclase para el test.
    # ASUMAMOS por ahora que DenseFaissRetriever puede tomar cualquier EmbedderPort.
    # Si no, este es un punto a refactorizar en DenseFaissRetriever.
    
    # Refactor Propuesto para DenseFaissRetriever.__init__:
    # def __init__(self, embedder: EmbedderPort): # En lugar de SentenceTransformerEmbedder
    
    retriever = DenseFaissRetriever(embedder=embedder) # Pasamos nuestro DummyEmbedder

    query_text = "query_target_doc1" # Esta query está diseñada para estar cerca de doc1_text (ID 101)
    
    # Recordar que RetrieverPort ahora devuelve (ids, scores)
    retrieved_ids, retrieved_scores = retriever.retrieve(query_text, k=1)

    assert len(retrieved_ids) == 1
    assert retrieved_ids[0] == 101 # El ID de "doc1_text"
    assert len(retrieved_scores) == 1
    assert isinstance(retrieved_scores[0], float)
    # Para L2, el score (distancia) debería ser pequeño
    assert retrieved_scores[0] < 0.1 # Ajustar este umbral basado en los embeddings de prueba

def test_faiss_respects_k(faiss_test_artefacts, monkeypatch):
    index_path, id_map_path, predefined_embeddings, dim = faiss_test_artefacts
    monkeypatch.setattr(global_settings, "index_path", str(index_path))
    monkeypatch.setattr(global_settings, "id_map_path", str(id_map_path))

    embedder = DummyEmbedder(dim=dim, predefined_embeddings=predefined_embeddings)
    retriever = DenseFaissRetriever(embedder=embedder)

    query_text = "query_target_doc1" # Query que podría coincidir con varios si k es mayor
    k_val = 2
    retrieved_ids, retrieved_scores = retriever.retrieve(query_text, k=k_val)

    assert len(retrieved_ids) == k_val
    assert len(retrieved_scores) == k_val
    assert 101 in retrieved_ids # doc1 debería estar
    # El segundo podría ser doc2 o doc3 dependiendo de las distancias exactas

def test_faiss_no_match(faiss_test_artefacts, monkeypatch):
    index_path, id_map_path, predefined_embeddings, dim = faiss_test_artefacts
    monkeypatch.setattr(global_settings, "index_path", str(index_path))
    monkeypatch.setattr(global_settings, "id_map_path", str(id_map_path))

    embedder = DummyEmbedder(dim=dim, predefined_embeddings=predefined_embeddings)
    retriever = DenseFaissRetriever(embedder=embedder)

    query_text = "query_no_match" # Diseñada para no coincidir bien
    retrieved_ids, retrieved_scores = retriever.retrieve(query_text, k=1)

    assert isinstance(retrieved_ids, list)
    assert isinstance(retrieved_scores, list)
    assert len(retrieved_ids) == len(retrieved_scores)
    if retrieved_ids: # Si devuelve algo
        assert len(retrieved_ids) <=1
        # La distancia para "query_no_match" (vector de ceros) a los otros vectores será mayor
        # que la distancia de "query_target_doc1" a "doc1_text".
        # Puedes añadir un assert sobre el valor del score si es predecible.
        # Por ejemplo, si retrieved_scores[0] > algún umbral alto.


# Más tests posibles:
# - Qué pasa si k es mayor que el número de documentos en el índice.
# - Qué pasa con una query vacía (si el DummyEmbedder la maneja de forma específica).


================================================================================
FILE: tests/unit/test_build.py
================================================================================

# tests/unit/test_build.py
from pathlib import Path
import csv
import importlib
import pickle

import faiss
from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker

from src.settings import settings as st
import src.db.base as db_base
from src.db.models import Document
import scripts.build_index as build_index


# ---------- helper ----------
def _count_documents(url: str) -> int:
    engine = create_engine(url)
    Session = sessionmaker(bind=engine)
    with Session() as s:
        return s.query(Document).count()


# ---------- tests ----------
def test_build_index_sparse(tmp_path: Path, monkeypatch):
    db_file = tmp_path / "app_sparse.db"
    csv_file = tmp_path / "kb_sparse.csv"

    rows = [["question", "answer"], ["q1", "a1"], ["q2", "a2"]]
    with csv_file.open("w", newline="", encoding="utf-8") as fh:
        csv.writer(fh, delimiter=";").writerows(rows)

    # patch settings
    monkeypatch.setattr(st, "faq_csv", str(csv_file))
    monkeypatch.setattr(st, "csv_has_header", True)
    monkeypatch.setattr(st, "sqlite_url", f"sqlite:///{db_file}")
    monkeypatch.setattr(st, "retrieval_mode", "sparse")

    importlib.reload(db_base)  # refrezca engine de src.db.base

    build_index.main()

    assert _count_documents(f"sqlite:///{db_file}") == 2  # excluye cabecera


def test_build_index_dense(tmp_path: Path, monkeypatch):
    db_file = tmp_path / "app_dense.db"
    csv_file = tmp_path / "kb_dense.csv"
    index_file = tmp_path / "index.faiss"
    id_map_file = tmp_path / "id_map.pkl"

    rows = [["question", "answer"], ["d1", "a1"], ["d2", "a2"]]
    with csv_file.open("w", newline="", encoding="utf-8") as fh:
        csv.writer(fh, delimiter=";").writerows(rows)

    monkeypatch.setattr(st, "faq_csv", str(csv_file))
    monkeypatch.setattr(st, "csv_has_header", True)
    monkeypatch.setattr(st, "sqlite_url", f"sqlite:///{db_file}")
    monkeypatch.setattr(st, "index_path", str(index_file))
    monkeypatch.setattr(st, "id_map_path", str(id_map_file))
    monkeypatch.setattr(st, "retrieval_mode", "dense")

    importlib.reload(db_base)

    build_index.main()

    # DB
    assert _count_documents(f"sqlite:///{db_file}") == 2

    # archivos FAISS
    assert index_file.exists()
    assert id_map_file.exists()

    faiss_index = faiss.read_index(str(index_file))
    with open(id_map_file, "rb") as fh:
        ids = pickle.load(fh)

    assert faiss_index.ntotal == 2
    assert len(ids) == 2



================================================================================
FILE: tests/unit/test_dependencies.py
================================================================================

# tests/unit/app/test_dependencies.py
import pytest
from unittest.mock import patch, MagicMock, call
import importlib
from pathlib import Path
import csv
import requests
import logging
from unittest import mock # Add this import

# Importar el módulo a testear y los settings para monkeypatch
from src.app import dependencies as app_dependencies_module
from src.settings import settings as global_settings # El objeto 'settings' real
from src.adapters.generation.openai_chat import OpenAIGenerator
from src.adapters.generation.ollama_chat import OllamaGenerator
from src.adapters.retrieval.sparse_bm25 import SparseBM25Retriever
from src.db.base import SessionLocal as AppSessionLocal # La SessionLocal de la app
from src.db.models import Document as DbDocument # El modelo SQLAlchemy


# --- Fixtures Específicas para este Módulo de Test ---
@pytest.fixture(autouse=True)
def reset_rag_service_singleton():
    """Asegura que el singleton _rag_service se resetea antes de cada test."""
    app_dependencies_module._rag_service = None
    yield
    app_dependencies_module._rag_service = None


@pytest.fixture
def mock_requests_get_fixture():
    with patch("requests.get") as mock_get:
        yield mock_get

@pytest.fixture
def mock_path_is_file_fixture():
    with patch.object(Path, 'is_file') as mock_is_file:
        yield mock_is_file

@pytest.fixture
def mock_logging_fixture():
    # Mockear los loggers específicos usados en dependencies.py
    # El nombre del logger es usualmente el nombre del módulo.
    with patch("src.app.dependencies.logging") as mock_log_module:
        # Puedes devolver un mock_logger específico si necesitas aserciones en él
        # mock_logger_instance = MagicMock()
        # mock_log_module.getLogger.return_value = mock_logger_instance
        yield mock_log_module # o mock_logger_instance


@pytest.fixture
def mock_dependencies_logger():
    with patch("src.app.dependencies.logger") as mock_log:
        yield mock_log

# --- Tests para _choose_generator ---

def test_choose_generator_ollama_enabled_and_works(monkeypatch, mock_requests_get_fixture):
    monkeypatch.setattr(global_settings, "ollama_enabled", True)
    monkeypatch.setattr(global_settings, "openai_api_key", None) # Sin OpenAI
    mock_requests_get_fixture.return_value = MagicMock(status_code=200)

    # Recargar el módulo para que use los settings parcheados si _choose_generator los lee directamente
    # o si importa OllamaGenerator/OpenAIGenerator que leen settings en su __init__.
    importlib.reload(app_dependencies_module)
    generator = app_dependencies_module._choose_generator()

    assert isinstance(generator, OllamaGenerator)
    mock_requests_get_fixture.assert_called_once_with(
        f"{global_settings.ollama_base_url.rstrip('/')}/api/tags", timeout=2
    )

def test_choose_generator_ollama_enabled_primary_fails_openai_works(monkeypatch, mock_requests_get_fixture):
    monkeypatch.setattr(global_settings, "ollama_enabled", True)
    monkeypatch.setattr(global_settings, "openai_api_key", "sk-testkey")
    # Falla la primera llamada a Ollama (health check)
    mock_requests_get_fixture.side_effect = requests.exceptions.ConnectionError("Ollama down")

    importlib.reload(app_dependencies_module)
    generator = app_dependencies_module._choose_generator()

    assert isinstance(generator, OpenAIGenerator)
    # Debería haber intentado Ollama primero
    mock_requests_get_fixture.assert_called_once_with(
        f"{global_settings.ollama_base_url.rstrip('/')}/api/tags", timeout=2
    )

def test_choose_generator_ollama_disabled_openai_works(monkeypatch, mock_requests_get_fixture):
    monkeypatch.setattr(global_settings, "ollama_enabled", False)
    monkeypatch.setattr(global_settings, "openai_api_key", "sk-testkey")

    importlib.reload(app_dependencies_module)
    generator = app_dependencies_module._choose_generator()

    assert isinstance(generator, OpenAIGenerator)
    mock_requests_get_fixture.assert_not_called() # No debería intentar Ollama si está deshabilitado

def test_choose_generator_ollama_primary_fails_no_openai_key_ollama_fallback_works(monkeypatch, mock_requests_get_fixture):
    monkeypatch.setattr(global_settings, "ollama_enabled", True)
    monkeypatch.setattr(global_settings, "openai_api_key", None)
    
    # Primera llamada (primary check) falla, segunda llamada (fallback check) funciona
    mock_requests_get_fixture.side_effect = [
        requests.exceptions.Timeout("Ollama primary timeout"),
        MagicMock(status_code=200)
    ]

    importlib.reload(app_dependencies_module)
    generator = app_dependencies_module._choose_generator()

    assert isinstance(generator, OllamaGenerator)
    assert mock_requests_get_fixture.call_count == 2
    expected_ollama_tags_url = f"{global_settings.ollama_base_url.rstrip('/')}/api/tags"
    mock_requests_get_fixture.assert_has_calls([
        call(expected_ollama_tags_url, timeout=2), # Primary check
        call(expected_ollama_tags_url, timeout=2)  # Fallback check
    ])

def test_choose_generator_all_fail_raises_runtime_error(monkeypatch, mock_requests_get_fixture):
    monkeypatch.setattr(global_settings, "ollama_enabled", True) # Intentará Ollama
    monkeypatch.setattr(global_settings, "openai_api_key", None) # No hay OpenAI
    mock_requests_get_fixture.side_effect = requests.exceptions.RequestException("Ollama always fails")

    importlib.reload(app_dependencies_module)
    with pytest.raises(RuntimeError, match="LLM Generator could not be initialized"):
        app_dependencies_module._choose_generator()
    
    # Debería haber intentado Ollama dos veces (primary y fallback)
    assert mock_requests_get_fixture.call_count == 2


# --- Tests para init_rag_service ---
# Estos tests necesitarán una base de datos en memoria.
# El `conftest.py` de nivel superior debería configurar esto para los tests de integración.
# Si este test_dependencies.py está en `tests/unit/app/`, `conftest.py` global
# (en `tests/`) podría no aplicar su magia de BD en memoria de la misma forma.
# ASUMIMOS que `conftest.py` en `tests/` ya configura una BD en memoria y parchea `settings.sqlite_url`.


def test_init_rag_service_dense_mode_fallback_to_sparse_if_files_missing(
    monkeypatch,
    mock_path_is_file_fixture, # Esta fixture con patch.object(Path, 'is_file') está bien
    caplog, # <-------------------- USA CAPLOG
    reset_rag_service_singleton
):
    # Arrange
    with AppSessionLocal() as db:
        db.query(DbDocument).delete()
        db.add_all([DbDocument(content="doc1"), DbDocument(content="doc2")])
        db.commit()

    monkeypatch.setattr(global_settings, "retrieval_mode", "dense")
    mock_path_is_file_fixture.return_value = False # is_file() mockeado siempre devolverá False
    monkeypatch.setattr(global_settings, "openai_api_key", "sk-testkey")
    monkeypatch.setattr(global_settings, "csv_has_header", True) # Asegúrate que esto esté si la lógica lo necesita

    # CONFIGURA CAPLOG
    # El logger en src/app/dependencies.py se llama 'src.app.dependencies'
    caplog.set_level(logging.WARNING, logger="src.app.dependencies")

    # importlib.reload(app_dependencies_module)

    # Imprime la configuración del engine de la SessionLocal que usará init_rag_service
    from src.db.base import SessionLocal as SL_in_deps # La que importa dependencies
    print(f"DEBUG TEST - Engine URL from SessionLocal in deps: {SL_in_deps.kw['bind'].url}")

    # Act
    app_dependencies_module.init_rag_service()
    service = app_dependencies_module.get_rag_service()

    # Assert
    assert service is not None
    assert isinstance(service.retriever, SparseBM25Retriever)

    found_warning_log = False
    # print(f"Caplog text for fallback test: {caplog.text}") # Para depurar
    for record in caplog.records:
        if record.name == "src.app.dependencies" and \
           record.levelname == "WARNING" and \
           "Falling back to sparse retrieval" in record.message:
            found_warning_log = True
            break
    assert found_warning_log, f"Expected fallback warning log was not found. Captured logs:\n{caplog.text}"

def test_init_rag_service_populates_db_from_csv_if_empty(
    monkeypatch,
    tmp_path: Path,
    caplog, # <------------------ AÑADE caplog COMO ARGUMENTO
    reset_rag_service_singleton
):
    # Arrange
    # Asegurar que la BD en memoria (parcheada por conftest.py) esté VACÍA
    with AppSessionLocal() as db:
        db.query(DbDocument).delete()
        db.commit()
        assert db.query(DbDocument).count() == 0 # Confirmar que está vacía

    # Crear un dummy_faq.csv
    csv_file_path = tmp_path / "dummy_faq.csv"
    csv_data = [
        ["question", "answer"],
        ["Q1", "A1: some keywords"],
        ["Q2", "A2: more data"],
    ]
    num_data_rows = len(csv_data) - 1

    with open(csv_file_path, "w", newline="", encoding="utf-8") as f:
        writer = csv.writer(f, delimiter=';')
        writer.writerows(csv_data)

    monkeypatch.setattr(global_settings, "faq_csv", str(csv_file_path))
    monkeypatch.setattr(global_settings, "csv_has_header", True)
    monkeypatch.setattr(global_settings, "openai_api_key", "sk-testkey")
    monkeypatch.setattr(global_settings, "retrieval_mode", "sparse")
    
    # Configura caplog ANTES de la acción que genera logs
    caplog.set_level(logging.INFO, logger="src.app.dependencies") # O el nombre correcto del logger

    importlib.reload(app_dependencies_module)

    # Act
    
    app_dependencies_module.init_rag_service()

    # Assert (DB count)
    with AppSessionLocal() as db_after_init:
        doc_count = db_after_init.query(DbDocument).count()
        print(f"DEBUG TEST: Count in DB AFTER init_rag_service call = {doc_count}") # Añade este print
        assert doc_count == num_data_rows # Debería ser 2

    # Assert (Log)
    found_population_log = False
    expected_log_part = f"DEPS: DB empty and FAQ CSV found; populating from {str(csv_file_path)}"
    # print(f"Caplog text for populate test: {caplog.text}") # Para depurar
    for record in caplog.records:
        # print(f"DEBUG LOG RECORD: name={record.name}, level={record.levelname}, msg='{record.message}'")
        if record.name == "src.app.dependencies" and \
            record.levelname == "INFO" and \
            expected_log_part in record.message: # More precise check
            found_population_log = True
            break
    assert found_population_log, f"Expected DB population info log part '{expected_log_part}' was not found. Captured logs:\n{caplog.text}"


def test_init_rag_service_does_not_populate_db_if_not_empty(
    monkeypatch,
    tmp_path: Path,
    caplog, # <------------------ AÑADE caplog COMO ARGUMENTO
    reset_rag_service_singleton
):
    # Arrange
    # Poblar la BD con un documento para que NO esté vacía
    initial_doc_content = "existing document"
    with AppSessionLocal() as db:
        db.query(DbDocument).delete()
        db.add(DbDocument(content=initial_doc_content))
        db.commit()
        assert db.query(DbDocument).count() == 1

    csv_file_path = tmp_path / "dummy_faq_ignored.csv"
    with open(csv_file_path, "w", newline="", encoding="utf-8") as f: # CSV no debería ser leído
        writer = csv.writer(f, delimiter=';')
        writer.writerow(["QH", "AH"])
        writer.writerow(["NewQ", "NewA"])

    monkeypatch.setattr(global_settings, "faq_csv", str(csv_file_path))
    monkeypatch.setattr(global_settings, "openai_api_key", "sk-testkey")
    monkeypatch.setattr(global_settings, "retrieval_mode", "sparse")

    caplog.set_level(logging.INFO, logger="src.app.dependencies") # O el nivel y logger que necesites
    importlib.reload(app_dependencies_module)

    # Act
    app_dependencies_module.init_rag_service() # Debería ver que la DB no está vacía

    # Assert
    # La DB no debería haber cambiado
    with AppSessionLocal() as db:
        doc_count = db.query(DbDocument).count()
        assert doc_count == 1 # Sigue siendo 1
        first_doc = db.query(DbDocument).first()
        assert first_doc.content == initial_doc_content

     # Assert (Log con caplog)
    found_population_log = False
    for record in caplog.records:
        if record.name == "src.app.dependencies" and \
           record.levelname == "INFO" and \
           "Populating from" in record.message: # No debería encontrar este log
            found_population_log = True
            break
    assert not found_population_log, f"DB population log found, but DB was not empty. Captured logs:\n{caplog.text}"


def test_get_rag_service_raises_assertion_error_if_not_initialized(reset_rag_service_singleton):
    # Asegurar que _rag_service es None (hecho por la fixture)
    assert app_dependencies_module._rag_service is None
    with pytest.raises(AssertionError, match="RagService not initialised"):
        app_dependencies_module.get_rag_service()


================================================================================
FILE: tests/unit/test_rag.py
================================================================================

# tests/unit/test_rag.py
"""
Unit-test del núcleo RagService sin depender de OpenAI ni BM25 reales,
y usando una base de datos en memoria para aislamiento.
"""
import pytest
from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker, Session # Importar Session explícitamente

# Importar Base y modelos directamente para usarlos con el engine en memoria
from src.db.base import Base # El Base original para definir la estructura de tablas
from src.db.models import Document as DbDocument # Alias para evitar confusión con un posible Document de Pydantic

from src.core.rag import RagService
from src.core.ports import RetrieverPort, GeneratorPort # Importar los Puertos

# --- Test Doubles (Stubs) ---
class DummyRetriever(RetrieverPort): # Implementar el Puerto
    def __init__(self, doc_ids: list[int] = None, scores: list[float] = None):
        self.doc_ids_to_return = doc_ids if doc_ids is not None else [1]
        self.scores_to_return = scores if scores is not None else [0.9] * len(self.doc_ids_to_return)
        if len(self.doc_ids_to_return) != len(self.scores_to_return):
            raise ValueError("doc_ids y scores deben tener la misma longitud en DummyRetriever")

    def retrieve(self, query: str, k: int = 5) -> tuple[list[int], list[float]]:
        # Devolver solo hasta k resultados
        return self.doc_ids_to_return[:k], self.scores_to_return[:k]


class DummyGenerator(GeneratorPort): # Implementar el Puerto
    def __init__(self, answer: str = "dummy answer"):
        self.answer_to_return = answer

    def generate(self, question: str, contexts: list[str]) -> str:
        # Podrías incluso hacer asserts sobre 'question' o 'contexts' aquí si fuera necesario
        return self.answer_to_return


# --- Fixture para la base de datos en memoria ---
@pytest.fixture(scope="function") # 'function' scope para una BD limpia por test
def db_session() -> Session: # Tipar el retorno para claridad
    # Crear un engine SQLite en memoria para este test
    engine = create_engine("sqlite:///:memory:")
    # Crear todas las tablas definidas en Base.metadata (ej. Document, QaHistory)
    Base.metadata.create_all(bind=engine)

    # Crear una SessionLocal específica para este engine en memoria
    TestingSessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)
    
    db = TestingSessionLocal()
    try:
        yield db # Proporcionar la sesión al test
    finally:
        db.close()
        Base.metadata.drop_all(bind=engine) # Limpiar después del test


# --- Tests ---
def test_rag_service_ask_retrieves_and_generates(db_session: Session):
    """
    Test que RagService.ask:
    1. Llama a retriever.retrieve().
    2. Llama a crud.get_documents() con los IDs del retriever.
    3. Llama a generator.generate() con la pregunta y los contextos.
    4. Devuelve la respuesta del generador y los IDs fuente.
    """
    # 1. Poblar la BD en memoria con un documento de prueba
    test_doc_id = 1
    test_doc_content = "dummy context content"
    doc = DbDocument(id=test_doc_id, content=test_doc_content)
    db_session.add(doc)
    db_session.commit()

    # 2. Configurar los Test Doubles
    retriever = DummyRetriever(doc_ids=[test_doc_id], scores=[0.95])
    generator = DummyGenerator(answer="specific dummy answer for this test")
    
    rag_service = RagService(retriever, generator)
    
    question = "hello?"

    # Act
    result = rag_service.ask(db_session, question)
    
    # Assert
    assert result["answer"] == "specific dummy answer for this test"
    assert result["source_ids"] == [test_doc_id]
    # Opcional: verificar que el historial se guardó (si QaHistory está definido en Base.metadata)
    # from src.db.models import QaHistory
    # history_entry = db_session.query(QaHistory).first()
    # assert history_entry is not None
    # assert history_entry.question == question
    # assert history_entry.answer == "specific dummy answer for this test"


def test_rag_service_ask_no_documents_found(db_session: Session):
    """
    Test que RagService.ask maneja el caso donde el retriever no devuelve IDs
    o los IDs no corresponden a documentos en la BD.
    El generador debería ser llamado con un contexto vacío.
    """
    # Arrange
    # BD está vacía (o los IDs devueltos no existen)
    
    retriever = DummyRetriever(doc_ids=[], scores=[]) # Retriever no devuelve nada
    
    # Queremos asegurar que el generador se llama con contextos vacíos
    class GeneratorSpy(GeneratorPort):
        called_with_contexts: list[str] | None = None
        def generate(self, question: str, contexts: list[str]) -> str:
            self.called_with_contexts = contexts
            return "generated with no context"

    generator_spy = GeneratorSpy()
    rag_service = RagService(retriever, generator_spy)
    question = "any question"

    # Act
    result = rag_service.ask(db_session, question)

    # Assert
    assert result["answer"] == "generated with no context"
    assert result["source_ids"] == []
    assert generator_spy.called_with_contexts == [] # Verifica que el contexto estaba vacío


def test_rag_service_uses_k_for_retrieval(db_session: Session):
    """
    Test que RagService.ask pasa el parámetro 'k' (o su default) al retriever.
    """
    # Arrange
    # Poblar la BD con suficientes documentos para que 'k' importe
    for i in range(1, 6):
        db_session.add(DbDocument(id=i, content=f"doc {i}"))
    db_session.commit()

    class RetrieverSpy(RetrieverPort):
        called_with_k: int | None = None
        def retrieve(self, query: str, k: int = 5) -> tuple[list[int], list[float]]:
            self.called_with_k = k
            # Devolver algunos IDs y scores válidos para que el flujo continúe
            actual_ids = [1, 2, 3, 4, 5]
            return actual_ids[:k], [0.9] * k 

    retriever_spy = RetrieverSpy()
    generator = DummyGenerator() # No nos importa la respuesta aquí
    rag_service = RagService(retriever_spy, generator)
    
    # Act
    rag_service.ask(db_session, "test question", k=3) # Llamar con k=3

    # Assert
    assert retriever_spy.called_with_k == 3

    # Act: Llamar sin k explícito (debería usar el default de RagService.ask, que es 3)
    rag_service.ask(db_session, "another test question")
    assert retriever_spy.called_with_k == 3 # El default de RagService.ask es k=3.

