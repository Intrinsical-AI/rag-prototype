
================================================================================
FILE: tests/conftest.py
================================================================================

# tests/conftest.py (o tests/integration/conftest.py si solo aplica a integración)
import sys
from pathlib import Path
import pytest
from sqlalchemy import create_engine
from sqlalchemy.pool import StaticPool # MUY IMPORTANTE para SQLite en memoria con FastAPI/multithreading

# --- 1. Add src to PYTHONPATH ---
# Assurs'from src...' working on tests/app when imported
PROJECT_ROOT = Path(__file__).resolve().parents[1]
SRC_PATH = PROJECT_ROOT / "src"
if SRC_PATH.is_dir():
    sys.path.insert(0, str(PROJECT_ROOT)) # Añadir el directorio PADRE de src
else:
    pass 

# --- 2. Parchear Settings Globalmente para la Sesión de Test ANTES de importar la app o dependencias ---
from src.settings import settings

# Memmory BD, default sparse mode, dummy API keys
settings.sqlite_url = "sqlite:///:memory:"      # memory db for all the tests
settings.retrieval_mode = "sparse"              # default for tests
settings.openai_api_key = "sk-dummy-test-key"   # avoid silly errores when OpenAI()
settings.ollama_enabled = False                 # Default para tests

# --- 3. Reconfigurar el Engine y SessionLocal de src.db.base para usar la BD en memoria ---
# Importa db_base AFTER settings.sqlite_url being patched
import src.db.base as db_base_module

# Crear un nuevo engine que apunte a la BD en memoria con StaticPool
# StaticPool es crucial para SQLite en memoria en contextos multi-hilo/async como FastAPI.
# Mantiene una única conexión subyacente por "hilo" (o en este caso, para el pool).
test_engine = create_engine(
    settings.sqlite_url, # Ya es "sqlite:///:memory:"
    connect_args={"check_same_thread": False}, # Necesario para SQLite
    poolclass=StaticPool, # Usar StaticPool
)

# Sobrescribir el engine global en el módulo db_base
db_base_module.engine = test_engine
# Reconfigurar la SessionLocal global para que use el nuevo test_engine
db_base_module.SessionLocal.configure(bind=test_engine)


# --- 4. Crear Esquema de BD y Poblar con Datos de Prueba Conocidos ---
# Importar Base y modelos DESPUÉS de reconfigurar db_base_module.engine
from src.db.models import Base as AppDeclarativeBase # El Base de tus modelos
from src.db.models import Document as DbDocument     # Tu modelo Document

# Crear todas las tablas definidas en AppDeclarativeBase (Document, QaHistory)
AppDeclarativeBase.metadata.create_all(bind=test_engine)

# Poblar la tabla Document con datos conocidos para los tests de integración
# Estos datos deben ser consistentes con lo que los tests esperan recuperar.
# Los IDs aquí son explícitos. Si build_index.py se usara, los IDs serían autoincrementales.
# Para tests de integración donde NO probamos build_index.py, es mejor setear datos explícitos.
with db_base_module.SessionLocal() as db:
    # Limpiar datos de ejecuciones anteriores si la BD no fuera puramente en memoria
    # (StaticPool con :memory: debería ser limpia cada vez, pero por si acaso)
    db.query(DbDocument).delete()
    db.commit()

    # Datos de prueba consistentes con las aserciones en test_api_ask.py
    test_docs_data = [
        {"id": 1, "content": "The refund policy states you can request a refund within 14 days."},
        {"id": 2, "content": "To contact support, please email support@example.com."},
        {"id": 3, "content": "Available features include semantic search and document processing."},
        # Añade más si es necesario para tus tests de retrieval
    ]
    for doc_data in test_docs_data:
        db.add(DbDocument(**doc_data))
    db.commit()

# --- 5. Fixture para Inicializar RagService una vez por sesión con la config de test ---
@pytest.fixture(scope="session", autouse=True)
def initialize_rag_service_for_session():
    """
    Esta fixture se ejecuta una vez por sesión de test, automáticamente.
    Llama a init_rag_service DESPUÉS de que todos los settings y la BD
    hayan sido configurados para el entorno de test.
    """
    # Importar el módulo de dependencias y llamar a init_rag_service
    # Esto asegura que el _rag_service singleton se crea usando los settings parcheados
    # y la BD en memoria ya poblada.
    from src.app import dependencies as app_dependencies
    
    # Es importante que init_rag_service() use el SessionLocal y Document
    # que ahora están vinculados al test_engine y la BD en memoria.
    # El reload de app_dependencies podría ser necesario si este importa settings
    # o db_base a nivel de módulo y queremos que relea los valores parcheados.
    # Sin embargo, como los settings y db_base.engine/SessionLocal se parchean
    # *antes* de esta fixture (a nivel de módulo de conftest), la primera importación
    # de app_dependencies ya debería ver los valores correctos.
    # Si init_rag_service en sí mismo importa settings o db_base directamente, también está bien.
    
    # importlib.reload(app_dependencies) # Opcional, probar sin él primero.
                                       # Podría ser necesario si app_dependencies
                                       # captura settings en el momento de su primera importación.
    
    app_dependencies.init_rag_service() # Esto creará el _rag_service global
    yield
    # No se necesita limpieza aquí si el _rag_service no mantiene recursos abiertos
    # que necesiten cierre explícito al final de la sesión.


================================================================================
FILE: tests/integration/test_api_ask.py
================================================================================

# tests/integration/test_api_ask.py
import pytest
from fastapi.testclient import TestClient
from unittest import mock
import random
import importlib # Para recargar módulos si es necesario

from src.settings import settings
from src.app.main import app # app se importa después de conftest.py
from src.app import dependencies as app_dependencies_module # Para re-init

# --- Fixture para el TestClient (ya no necesita ser module-scoped si la app es estable) ---
@pytest.fixture(scope="function") # 'function' scope puede ser más seguro si los tests modifican estado de la app
def client() -> TestClient:
    with TestClient(app) as c:
        yield c

# --- Fixture para limpiar la tabla de historial ---
@pytest.fixture(scope="function")
def clean_history_table():
    from src.db.base import SessionLocal # Usar el SessionLocal ya reconfigurado por conftest
    from src.db.models import QaHistory
    with SessionLocal() as db:
        db.query(QaHistory).delete()
        db.commit()
    yield

# --- Helper para mock de OpenAI API v1.x ---
def _make_openai_v1_mock(answer_text: str):
    mock_completion = mock.MagicMock()
    mock_completion.choices = [mock.MagicMock()]
    mock_completion.choices[0].message = mock.MagicMock()
    mock_completion.choices[0].message.content = answer_text
    return mock_completion

# --- Tests ---
@mock.patch("src.adapters.generation.openai_chat.OpenAI") # Path donde OpenAI es instanciado
def test_api_ask_with_openai_retrieves_and_generates(
    MockOpenAIClass,
    client: TestClient,
    monkeypatch
):
    # Arrange
    # 1. Asegurar que se usará OpenAIGenerator y que tiene una API key para __init__
    monkeypatch.setattr(settings, "ollama_enabled", False)
    current_openai_api_key = settings.openai_api_key # Guardar por si se parchea
    if settings.openai_api_key is None or settings.openai_api_key == "sk-dummy-test-key": # Evitar sobreescribir una real
        monkeypatch.setattr(settings, "openai_api_key", "sk-integration-openai-ask")

    # 2. Configurar el mock para la clase OpenAI
    mock_openai_client_instance = MockOpenAIClass.return_value
    expected_answer = "Mocked AI answer about our refund policy."
    mock_openai_client_instance.chat.completions.create.return_value = _make_openai_v1_mock(expected_answer)

    # 3. Forzar la re-inicialización de RagService para que use el OpenAIGenerator
    #    que a su vez usará la clase OpenAI mockeada.
    #    Y para que el retriever se cree con el modo correcto si lo cambiamos.
    monkeypatch.setattr(settings, "retrieval_mode", "sparse") # Asegurar modo para aserción de source_id
    importlib.reload(app_dependencies_module) # Recargar para que vea los settings parcheados
    app_dependencies_module.init_rag_service() # Recrea _rag_service

    question_text = "What is the refund policy?"
    payload = {"question": question_text}

    # Act
    response = client.post("/api/ask", json=payload)

    # Assert
    assert response.status_code == 200
    body = response.json()
    assert body["answer"] == expected_answer
    assert isinstance(body["source_ids"], list)
    assert 1 in body["source_ids"] # Basado en datos de prueba en conftest.py

    mock_openai_client_instance.chat.completions.create.assert_called_once()
    args, kwargs = mock_openai_client_instance.chat.completions.create.call_args
    sent_prompt_content = kwargs["messages"][0]["content"]
    assert question_text in sent_prompt_content
    assert "refund policy states" in sent_prompt_content.lower() # Contexto del Doc ID 1

    # Restaurar API key si se cambió, aunque monkeypatch debería hacerlo por test
    monkeypatch.setattr(settings, "openai_api_key", current_openai_api_key)


@mock.patch("requests.post") # Path donde requests.post es llamado por OllamaGenerator
def test_api_ask_with_ollama_retrieves_and_generates(
    mock_requests_post,
    client: TestClient,
    monkeypatch
):
    # Arrange
    # 1. Habilitar Ollama y deshabilitar retrieval denso para simplificar
    monkeypatch.setattr(settings, "ollama_enabled", True)
    monkeypatch.setattr(settings, "retrieval_mode", "sparse") # O el modo que quieras probar

    # 2. Configurar mock para requests.post (Ollama)
    expected_answer = "Mocked Ollama answer regarding support."
    mock_ollama_response_obj = mock.MagicMock()
    mock_ollama_response_obj.json.return_value = {"response": expected_answer}
    mock_ollama_response_obj.status_code = 200
    mock_ollama_response_obj.raise_for_status.return_value = None
    mock_requests_post.return_value = mock_ollama_response_obj

    # 3. Forzar la re-inicialización de RagService para que use OllamaGenerator
    #    y el retriever correcto.
    importlib.reload(app_dependencies_module)
    app_dependencies_module.init_rag_service()

    question_text = "How to contact support?"
    payload = {"question": question_text}

    # Act
    response = client.post("/api/ask", json=payload)

    # Assert
    assert response.status_code == 200
    body = response.json()
    assert body["answer"] == expected_answer
    assert 2 in body["source_ids"] # Basado en datos de prueba en conftest.py

    mock_requests_post.assert_called_once()
    args, kwargs = mock_requests_post.call_args
    sent_payload = kwargs["json"]
    assert question_text in sent_payload["prompt"]
    assert "contact support, please email" in sent_payload["prompt"].lower() # Contexto del Doc ID 2


def test_api_ask_validation_error_missing_question(client: TestClient):
    response = client.post("/api/ask", json={})
    assert response.status_code == 422
    detail = response.json().get("detail", [])
    assert any("question" in error.get("loc", []) for error in detail if isinstance(error, dict) and "loc" in error)


def test_api_ask_validation_error_wrong_type(client: TestClient):
    response = client.post("/api/ask", json={"question": 123})
    assert response.status_code == 422
    detail = response.json().get("detail", [])
    assert any("Input should be a valid string" in error.get("msg", "") for error in detail if isinstance(error, dict) and "msg" in error)


@mock.patch("src.adapters.generation.openai_chat.OpenAI")
def test_history_endpoint_records_and_retrieves_qa(
    MockOpenAIClass,
    client: TestClient,
    monkeypatch,
    clean_history_table # Usar fixture para limpiar tabla
):
    # Arrange
    # 1. Configurar para usar OpenAIGenerator con mock
    monkeypatch.setattr(settings, "ollama_enabled", False)
    current_openai_api_key = settings.openai_api_key
    if settings.openai_api_key is None or settings.openai_api_key == "sk-dummy-test-key":
        monkeypatch.setattr(settings, "openai_api_key", "sk-integration-hist-test")
    
    mock_openai_client_instance = MockOpenAIClass.return_value

    # Forzar re-init de RagService para asegurar que usa estos settings
    # (aunque conftest.py ya lo hace una vez, ollama_enabled podría haber cambiado)
    importlib.reload(app_dependencies_module)
    app_dependencies_module.init_rag_service()

    # Primera Q&A
    q_text1 = f"History Q1 {random.randint(1000, 9999)}"
    ans_text1 = "Hist Ans 1"
    mock_openai_client_instance.chat.completions.create.return_value = _make_openai_v1_mock(ans_text1)
    resp1 = client.post("/api/ask", json={"question": q_text1})
    assert resp1.status_code == 200

    # Segunda Q&A
    q_text2 = f"History Q2 {random.randint(1000, 9999)}"
    ans_text2 = "Hist Ans 2"
    # Importante: .create es un mock, si la misma instancia de mock_openai_client_instance se usa,
    # su return_value para .create necesita ser reconfigurado si la respuesta cambia.
    mock_openai_client_instance.chat.completions.create.return_value = _make_openai_v1_mock(ans_text2)
    resp2 = client.post("/api/ask", json={"question": q_text2})
    assert resp2.status_code == 200
    
    # Act
    resp_hist = client.get("/api/history?limit=5")
    assert resp_hist.status_code == 200
    history_data = resp_hist.json()

    # Assert
    assert len(history_data) == 2 # Exactamente 2 debido a clean_history_table

    assert history_data[0]["question"] == q_text2 # Más reciente primero
    assert history_data[0]["answer"] == ans_text2
    assert history_data[1]["question"] == q_text1
    assert history_data[1]["answer"] == ans_text1

    assert mock_openai_client_instance.chat.completions.create.call_count == 2

    monkeypatch.setattr(settings, "openai_api_key", current_openai_api_key)


================================================================================
FILE: tests/unit/adapters/embeddings/test_openai_embeddings.py
================================================================================

# tests/unit/adapters/embeddings/test_openai_embeddings.py
import pytest
from unittest import mock
from openai import APIError # Importar el error de la API v1.x

from src.adapters.embeddings.openai import OpenAIEmbedder
from src.settings import settings # Para verificar el modelo usado

# No necesitamos la fixture openai_embedder_instance si instanciamos dentro de cada test
# donde el mock de la clase OpenAI está activo.

@mock.patch("src.adapters.embeddings.openai.OpenAI") # Mockea la CLASE OpenAI
def test_openai_embedder_happy_path(MockOpenAI, monkeypatch):
    # Arrange
    if settings.openai_api_key is None: # Asegurar API key para el __init__ del embedder
        monkeypatch.setattr(settings, "openai_api_key", "sk-dummy-for-happy-path")

    text_to_embed = "This is a test sentence for happy path."
    # Crear un vector de la dimensión correcta (1536 para text-embedding-3-small por defecto en settings)
    # Usamos el DIM del embedder que se instancia para asegurar consistencia.
    # La instanciación de OpenAIEmbedder debe ocurrir después de cualquier monkeypatch de settings.openai_embedding_model
    # para que DIM se calcule correctamente. Aquí usamos el DIM por defecto.
    
    # Primero instanciamos el embedder para obtener su DIM configurado
    # (asumiendo que settings.openai_embedding_model no se cambia para este test)
    temp_embedder_for_dim = OpenAIEmbedder()
    expected_dim = temp_embedder_for_dim.DIM
    expected_embedding_vector = [i * 0.001 for i in range(expected_dim)]


    mock_client_instance = MockOpenAI.return_value # La instancia que OpenAI() devolvería

    # Configurar el mock para que devuelva una estructura similar a la de OpenAI Embeddings API v1.x
    mock_embedding_api_response = mock.MagicMock()
    mock_embedding_data_item = mock.MagicMock()
    mock_embedding_data_item.embedding = expected_embedding_vector
    mock_embedding_api_response.data = [mock_embedding_data_item]
    
    mock_client_instance.embeddings.create.return_value = mock_embedding_api_response

    # Instanciar el embedder que realmente probaremos, DESPUÉS de que el mock esté activo
    # y después de cualquier monkeypatch que afecte a su __init__.
    embedder_under_test = OpenAIEmbedder() # Su __init__ llamará a MockOpenAI()

    # Act
    embedding_result = None
    try:
        embedding_result = embedder_under_test.embed(text_to_embed)
        mock_client_instance.embeddings.create.assert_called_once_with(
            model=settings.openai_embedding_model,
            input=text_to_embed
        )
    except Exception as e:
        pytest.fail(f"Embed en happy path falló inesperadamente: {e}")

    # Assert
    assert embedding_result == expected_embedding_vector
    assert embedder_under_test.DIM == expected_dim # Verificar que el DIM es el esperado


@mock.patch("src.adapters.embeddings.openai.OpenAI")
def test_openai_embedder_handles_api_error(MockOpenAI, monkeypatch):
    # Arrange
    if settings.openai_api_key is None:
        monkeypatch.setattr(settings, "openai_api_key", "sk-dummy-for-api-error")

    text_to_embed = "Sentence that will cause API error."
    
    mock_client_instance = MockOpenAI.return_value
    mock_client_instance.embeddings.create.side_effect = APIError(
        message="Mock Embedding API Error from test", request=None, body=None
    )

    # Instanciar el embedder que probaremos
    embedder_under_test = OpenAIEmbedder()

    # Act & Assert
    with pytest.raises(APIError) as exc_info:
        embedder_under_test.embed(text_to_embed)
    
    assert "Mock Embedding API Error from test" in str(exc_info.value)
    mock_client_instance.embeddings.create.assert_called_once_with(
        model=settings.openai_embedding_model, # Verificar que se intentó llamar con los params correctos
        input=text_to_embed
    )

@mock.patch("src.adapters.embeddings.openai.OpenAI")
def test_openai_embedder_dim_updates_with_model_setting(MockOpenAI, monkeypatch):
    # Arrange
    original_embedding_model = settings.openai_embedding_model # Guardar para restaurar
    
    # Caso 1: text-embedding-3-large
    monkeypatch.setattr(settings, "openai_embedding_model", "text-embedding-3-large")
    if settings.openai_api_key is None:
        monkeypatch.setattr(settings, "openai_api_key", "sk-dummy-for-dim-test-large")
    
    # MockOpenAI se aplica aquí, así que la instancia de OpenAI() será un mock
    embedder_large = OpenAIEmbedder() 
    assert embedder_large.DIM == 3072, "DIM should be 3072 for text-embedding-3-large"
    # No necesitamos verificar la llamada a .create() aquí, solo el __init__ y DIM.
    # MockOpenAI().assert_called_once() # Verifica que el constructor de OpenAI (mockeado) fue llamado.
                                      # Esto es un poco más avanzado, puede que necesites
                                      # mock_constructor = MockOpenAI; mock_constructor.assert_called_once()
                                      # o verificar que mock_client_instance fue "creado"
    MockOpenAI.assert_called_with(api_key=settings.openai_api_key) # Verificar args de OpenAI()

    # Limpiar el mock de llamada para el siguiente caso (si MockOpenAI fuera persistente entre llamadas)
    # pero como se re-instancia el embedder, MockOpenAI() se llama de nuevo.
    MockOpenAI.reset_mock() # Resetea conteos de llamadas, etc.

    # Caso 2: text-embedding-ada-002 (también tiene DIM 1536, pero probamos el setting)
    monkeypatch.setattr(settings, "openai_embedding_model", "text-embedding-ada-002")
    if settings.openai_api_key is None: # Redundante si ya se seteó, pero no daña
        monkeypatch.setattr(settings, "openai_api_key", "sk-dummy-for-dim-test-ada")

    embedder_ada = OpenAIEmbedder()
    assert embedder_ada.DIM == 1536, "DIM should be 1536 for text-embedding-ada-002"
    MockOpenAI.assert_called_with(api_key=settings.openai_api_key)

    # Restaurar el setting original para no afectar otros tests si se ejecutan en la misma sesión
    # (aunque pytest debería aislar, es buena práctica con monkeypatch a nivel de settings globales)
    monkeypatch.setattr(settings, "openai_embedding_model", original_embedding_model)


================================================================================
FILE: tests/unit/adapters/embeddings/test_sentence_transformer_embedder.py
================================================================================

# tests/unit/adapters/embeddings/test_sentence_transformer_embedder.py
from unittest import mock
import numpy as np # Para comparar arrays si es necesario

from src.adapters.embeddings.sentence_transformers import SentenceTransformerEmbedder
# Asumimos que SentenceTransformerEmbedder implementa EmbedderPort
# from src.core.ports import EmbedderPort 

# No necesitamos una fixture para instanciar SentenceTransformerEmbedder
# porque su __init__ es el que carga el modelo, y eso es lo que queremos mockear
# o controlar.

# El path para mockear es donde 'SentenceTransformer' (la clase de la librería)
# es importada y usada por tu adaptador.
@mock.patch("src.adapters.embeddings.sentence_transformers.SentenceTransformer")
def test_sentence_transformer_embedder_happy_path(MockSentenceTransformer):
    # Arrange
    model_name_expected = "all-MiniLM-L6-v2" # El default en tu adaptador
    text_to_embed = "This is a test sentence for sentence-transformer."
    
    # Configurar el mock de la instancia de SentenceTransformer y su método encode
    mock_model_instance = MockSentenceTransformer.return_value # Lo que SentenceTransformer(model_name) devolvería
    
    # SentenceTransformer().encode() devuelve un numpy array.
    # La DIM de tu adaptador es 384 para 'all-MiniLM-L6-v2'.
    expected_embedding_vector_np = np.array([i * 0.01 for i in range(384)], dtype=np.float32)
    # Tu adaptador convierte esto a una lista de Python floats.
    expected_embedding_list = expected_embedding_vector_np.tolist()
    
    mock_model_instance.encode.return_value = [expected_embedding_vector_np] # encode espera una lista de textos y devuelve una lista de arrays

    # Act
    # Instanciar el embedder. Su __init__ llamará a SentenceTransformer(model_name_expected),
    # que ahora está mockeado por MockSentenceTransformer.
    embedder = SentenceTransformerEmbedder(model_name=model_name_expected)
    embedding_result = embedder.embed(text_to_embed)

    # Assert
    # 1. Verificar que SentenceTransformer fue instanciado con el nombre de modelo correcto
    MockSentenceTransformer.assert_called_once_with(model_name_expected)
    
    # 2. Verificar que el método encode del modelo mockeado fue llamado correctamente
    mock_model_instance.encode.assert_called_once_with([text_to_embed]) # encode toma una lista de strings
    
    # 3. Verificar que el embedding devuelto es el esperado
    assert embedding_result == expected_embedding_list
    assert isinstance(embedding_result, list)
    assert len(embedding_result) == 384 # Verificar dimensión
    assert all(isinstance(x, float) for x in embedding_result) # Verificar tipo de los elementos

    # 4. Verificar el atributo DIM del embedder
    assert embedder.DIM == 384


@mock.patch("src.adapters.embeddings.sentence_transformers.SentenceTransformer")
def test_sentence_transformer_embedder_custom_model_name(MockSentenceTransformer):
    # Arrange
    custom_model_name = "paraphrase-multilingual-MiniLM-L12-v2"
    text_to_embed = "Otra frase de prueba."
    
    mock_model_instance = MockSentenceTransformer.return_value
    # Asumimos que este modelo también tiene DIM 384 para simplificar el mock,
    # aunque en realidad podría ser diferente. El test se enfoca en que el nombre del modelo se pase.
    # Si la DIM fuera diferente, el embedder.DIM debería reflejarlo (requeriría lógica en __init__ para actualizar DIM)
    # o el test debería mockear la DIM de forma diferente.
    # Por ahora, mantenemos la DIM de 384.
    expected_embedding_vector_np = np.array([i * 0.02 for i in range(384)], dtype=np.float32)
    expected_embedding_list = expected_embedding_vector_np.tolist()
    mock_model_instance.encode.return_value = [expected_embedding_vector_np]

    # Act
    embedder = SentenceTransformerEmbedder(model_name=custom_model_name) # Usar nombre custom
    embedding_result = embedder.embed(text_to_embed)

    # Assert
    MockSentenceTransformer.assert_called_once_with(custom_model_name) # Verificar nombre custom
    mock_model_instance.encode.assert_called_once_with([text_to_embed])
    assert embedding_result == expected_embedding_list
    # Nota: El atributo embedder.DIM se hardcodea a 384 en tu clase actualmente.
    # Si quisieras que fuera dinámico según el modelo, el __init__ necesitaría obtenerlo del modelo.
    # Para el test actual, el DIM seguirá siendo 384.
    assert embedder.DIM == 384 


================================================================================
FILE: tests/unit/adapters/generation/test_generation_ollama.py
================================================================================

# tests/unit/test_generation_ollama.py
import pytest
from unittest import mock
import requests # Para los tipos de excepciones
from fastapi import HTTPException

from src.adapters.generation.ollama_chat import OllamaGenerator
from src.settings import settings # Para las configuraciones

@pytest.fixture
def ollama_generator() -> OllamaGenerator:
    # Si necesitas mockear settings para este test, usa monkeypatch
    return OllamaGenerator()

@mock.patch("requests.post") # Mockea requests.post
def test_ollama_generator_happy_path(mock_post, ollama_generator: OllamaGenerator):
    # Arrange
    question = "Ollama test question?"
    contexts = ["Ollama context 1.", "Ollama context 2."]
    expected_ollama_answer = "This is a mock Ollama answer."

    mock_response_object = mock.MagicMock()
    mock_response_object.json.return_value = {"response": expected_ollama_answer}
    mock_response_object.status_code = 200
    mock_response_object.raise_for_status.return_value = None # No levanta error
    mock_post.return_value = mock_response_object

    # Act
    answer = ollama_generator.generate(question, contexts)

    # Assert
    assert answer == expected_ollama_answer
    mock_post.assert_called_once()
    
    args, kwargs = mock_post.call_args
    # print(args)
    # print(kwargs)
    
    expected_api_url = f"{settings.ollama_base_url.rstrip('/')}/api/generate"
    assert args[0] == expected_api_url
    
    payload = kwargs["json"]
    assert payload["model"] == settings.ollama_model
    assert payload["stream"] is False
    
    ctx_block_expected = "\n".join(f"- {c}" for c in contexts)
    full_prompt_expected = (
        "Based on the following context, please answer the question.\nIf the context does not provide an answer, say so.\n\n"
        "CONTEXT:\n"
        f"{ctx_block_expected}\n\n"
        "QUESTION:\n"
        f"{question}"
    )
    assert payload["prompt"] == full_prompt_expected
    assert kwargs["timeout"] == settings.ollama_request_timeout


@mock.patch("requests.post")
def test_ollama_generator_handles_connection_error(mock_post, ollama_generator: OllamaGenerator):
    # Arrange
    mock_post.side_effect = requests.exceptions.ConnectionError("Failed to connect")

    # Act & Assert
    with pytest.raises(HTTPException) as exc_info:
        ollama_generator.generate("q", ["c"])
    
    assert exc_info.value.status_code == 503
    assert "Could not connect to Ollama server" in exc_info.value.detail


@mock.patch("requests.post")
def test_ollama_generator_handles_http_error(mock_post, ollama_generator: OllamaGenerator):
    # Arrange
    mock_response_object = mock.MagicMock()
    mock_response_object.status_code = 500
    mock_response_object.text = "Internal Server Error from Ollama"
    mock_response_object.raise_for_status.side_effect = requests.exceptions.HTTPError(
        "Mock HTTP Error", response=mock_response_object
    )
    mock_post.return_value = mock_response_object
    
    # Act & Assert
    with pytest.raises(HTTPException) as exc_info:
        ollama_generator.generate("q", ["c"])
        
    assert exc_info.value.status_code == 500
    assert "Ollama API error: Internal Server Error from Ollama" in exc_info.value.detail

@mock.patch("requests.post")
def test_ollama_generator_handles_malformed_json_response(mock_post, ollama_generator: OllamaGenerator):
    # Arrange
    mock_response_object = mock.MagicMock()
    mock_response_object.status_code = 200
    mock_response_object.raise_for_status.return_value = None
    mock_response_object.json.return_value = {"error": "unexpected_format_no_response_key"} # Falta la clave 'response'
    mock_post.return_value = mock_response_object

    # Act & Assert
    with pytest.raises(HTTPException) as exc_info:
        ollama_generator.generate("q", ["c"])
    
    assert exc_info.value.status_code == 500
    assert "Ollama response malformed: 'response' key missing" in exc_info.value.detail


================================================================================
FILE: tests/unit/adapters/generation/test_generation_openai.py
================================================================================

# tests/unit/test_generation_openai.py
import pytest
from unittest import mock
from fastapi import HTTPException
from openai import APIError # Importar el nuevo error

from src.adapters.generation.openai_chat import OpenAIGenerator
from src.settings import settings

@pytest.fixture
def openai_generator_instance(monkeypatch) -> OpenAIGenerator:
    # Si settings.openai_api_key es None y confías en la var de entorno
    monkeypatch.setenv("OPENAI_API_KEY", "sk-dummy-test-key-for-env")
    generator = OpenAIGenerator()
    monkeypatch.delenv("OPENAI_API_KEY", raising=False) # Limpiar después
    return generator

# El path para mockear cambia a donde se encuentra el método que queremos interceptar.
# Como 'self.client' se crea en el __init__ de OpenAIGenerator, y luego se llama
# 'self.client.chat.completions.create', necesitamos mockear ese método.
# Esto se puede hacer mockeando la clase OpenAI y su cadena de atributos.
@mock.patch("src.adapters.generation.openai_chat.OpenAI") # Mockea la clase OpenAI donde se importa
def test_openai_generator_happy_path(MockOpenAI, openai_generator_instance: OpenAIGenerator):
    # Arrange
    question = "Test question?"
    contexts = ["Context 1.", "Context 2 snippet."]
    expected_generated_answer = "This is a mock AI answer for API v1."

    # Configurar el mock de la instancia del cliente y su método
    mock_client_instance = MockOpenAI.return_value # Esto es lo que self.client será
    mock_completion = mock.MagicMock()
    mock_completion.choices = [mock.MagicMock()]
    mock_completion.choices[0].message = mock.MagicMock()
    mock_completion.choices[0].message.content = expected_generated_answer
    
    # El método mockeado ahora es en la instancia del cliente
    mock_client_instance.chat.completions.create.return_value = mock_completion

    # Act
    # Necesitamos re-instanciar OpenAIGenerator DESPUÉS de que MockOpenAI esté activo
    # para que su __init__ use el cliente mockeado, O inyectar el cliente mockeado.
    # La forma más simple es que la fixture ya use el mock si es posible, o
    # que el generador se cree aquí.
    
    # Si OpenAIGenerator crea self.client en __init__, y la fixture lo crea antes del mock,
    # el cliente no será el mockeado.
    # Opción 1: Recrear el generador aquí (más simple para este test)
    generator_under_test = OpenAIGenerator() # Su __init__ ahora usará MockOpenAI().return_value

    answer = generator_under_test.generate(question, contexts)

    # Assert
    assert answer == expected_generated_answer

    # Verificar que el método create fue llamado una vez en la instancia mockeada del cliente
    mock_client_instance.chat.completions.create.assert_called_once()
    
    args, kwargs = mock_client_instance.chat.completions.create.call_args
    # print(kwargs) # Descomenta para ver los kwargs

    assert kwargs["model"] == settings.openai_model
    # La API Key ya no se pasa a 'create', se usa al instanciar el cliente OpenAI.
    # assert kwargs["api_key"] == settings.openai_api_key # YA NO ES ASÍ
    assert kwargs["temperature"] == settings.openai_temperature

    messages = kwargs["messages"]
    assert len(messages) == 1
    assert messages[0]["role"] == "user"
    
    ctx_block_expected = "\n".join(f"- {c}" for c in contexts)
    prompt_content_expected = (
        "Answer using ONLY the context provided.\n\n"
        f"CONTEXT:\n{ctx_block_expected}\n\n"
        f"QUESTION: {question}"
    )
    assert messages[0]["content"] == prompt_content_expected


@mock.patch("src.adapters.generation.openai_chat.OpenAI")
def test_openai_generator_handles_api_error(MockOpenAI, openai_generator_instance: OpenAIGenerator): # La fixture no se usa directamente aquí
    # Arrange
    question = "Another question"
    contexts = ["Some context."]
    
    mock_client_instance = MockOpenAI.return_value
    # Configurar el mock para que levante un APIError de OpenAI v1.x
    # El error puede tener un 'response' mockeado si tu código lo usa.
    # Para un error simple, un mensaje es suficiente.
    # Necesitas crear un objeto mock de respuesta si tu manejo de errores lo espera.
    # Por ahora, un APIError simple.
    mock_client_instance.chat.completions.create.side_effect = APIError(
        message="Mock API Error from v1", 
        request=None, # puedes mockear request si es necesario
        body=None     # puedes mockear body si es necesario
    )
    # Si necesitas simular un status_code específico, APIError lo puede tomar o se puede
    # construir un mock_response para el error.
    # error_response = mock.MagicMock()
    # error_response.status_code = 429 # Ejemplo
    # mock_client_instance.chat.completions.create.side_effect = APIError(
    #     message="Rate limit exceeded", request=None, body=None, response=error_response
    # )


    generator_under_test = OpenAIGenerator() # Para que use el cliente mockeado

    # Act & Assert
    with pytest.raises(HTTPException) as exc_info:
        generator_under_test.generate(question, contexts)
    
    assert exc_info.value.status_code == 502 # O el código que decidas para el error
    # El mensaje de error ahora vendrá de err.message del APIError
    assert "OpenAI API Error: Mock API Error from v1" in str(exc_info.value.detail)

    mock_client_instance.chat.completions.create.assert_called_once()


================================================================================
FILE: tests/unit/adapters/retrieval/test_retrieval_bm25.py
================================================================================

# tests/unit/test_retrieval_bm25.py
import pytest
from src.adapters.retrieval.sparse_bm25 import SparseBM25Retriever # o el path correcto

# --- Fixtures (si son necesarias) ---
@pytest.fixture
def sample_documents_data() -> tuple[list[str], list[int]]:
    documents = [
        "El perro marrón rápido salta sobre el zorro perezoso.",
        "Nunca la lluvia en España cae principalmente en la llanura.",
        "El perro es el mejor amigo del hombre.",
        "Los zorros son ágiles y los perros también.",
    ]
    doc_ids = [10, 20, 30, 40] # IDs arbitrarios para los documentos
    return documents, doc_ids

# --- Tests ---
def test_bm25_retrieves_relevant_doc(sample_documents_data):
    documents, doc_ids = sample_documents_data
    retriever = SparseBM25Retriever(documents=documents, doc_ids=doc_ids)
    
    query = "perro amigo"
    # Asumimos que `retrieve` ahora devuelve (ids, scores)
    retrieved_ids, retrieved_scores = retriever.retrieve(query, k=1)
    
    assert len(retrieved_ids) == 1
    assert retrieved_ids[0] == 30 # "El perro es el mejor amigo del hombre."
    assert len(retrieved_scores) == 1
    assert isinstance(retrieved_scores[0], float) # O el tipo que esperes

def test_bm25_respects_k(sample_documents_data):
    documents, doc_ids = sample_documents_data
    retriever = SparseBM25Retriever(documents=documents, doc_ids=doc_ids)
    
    query = "LES"
    k_val = 2
    retrieved_ids, retrieved_scores = retriever.retrieve(query, k=k_val)
    
    assert len(retrieved_ids) == k_val
    assert len(retrieved_scores) == k_val
    # Podrías hacer aserciones más específicas sobre los IDs esperados si el orden es predecible

def test_bm25_no_match(sample_documents_data):
    documents, doc_ids = sample_documents_data
    retriever = SparseBM25Retriever(documents=documents, doc_ids=doc_ids)
    
    query = "gato unicornio inexistente"
    retrieved_ids, retrieved_scores = retriever.retrieve(query, k=1)
    
    # BM25 aún podría devolver algo si hay solapamiento de tokens comunes
    # o podría devolver una lista vacía si los scores son muy bajos o cero.
    # Esto depende de la implementación de rank_bm25 y cómo manejas scores cero.
    # Si se esperan resultados (incluso con scores bajos):
    # assert len(retrieved_ids) > 0 # o un número específico
    # Si se espera una lista vacía para no coincidencias fuertes:
    # assert len(retrieved_ids) == 0
    # assert len(retrieved_scores) == 0
    # Por ahora, seamos flexibles y solo verifiquemos la estructura:
    assert isinstance(retrieved_ids, list)
    assert isinstance(retrieved_scores, list)
    assert len(retrieved_ids) == len(retrieved_scores)


def test_bm25_empty_query(sample_documents_data):
    documents, doc_ids = sample_documents_data
    retriever = SparseBM25Retriever(documents=documents, doc_ids=doc_ids)
    
    query = ""
    retrieved_ids, retrieved_scores = retriever.retrieve(query, k=1)

    # rank_bm25 con query vacía usualmente da scores de 0 para todos los docs.
    # Devolvería todos los documentos con score 0 si k es lo suficientemente grande,
    # o los primeros k documentos con score 0.
    assert len(retrieved_ids) <= 1 # Debería devolver k documentos o menos
    assert len(retrieved_scores) <= 1
    if retrieved_scores: # Si devuelve algo
        assert all(score == 0.0 for score in retrieved_scores)

# Podrías añadir más tests:
# - Con documentos vacíos
# - Verificando el orden de los scores (el primer score debe ser >= al segundo, etc.)


================================================================================
FILE: tests/unit/adapters/retrieval/test_retrieval_faiss.py
================================================================================

# tests/unit/test_retrieval_dense_faiss.py
import pytest
import numpy as np
import faiss
import pickle
from pathlib import Path

from src.settings import settings as global_settings # Para monkeypatch
from src.adapters.retrieval.dense_faiss import DenseFaissRetriever
from src.core.ports import EmbedderPort # Para nuestro DummyEmbedder

# --- Dummy Embedder ---
class DummyEmbedder(EmbedderPort):
    DIM: int # Debe coincidir con la dimensión del índice FAISS de prueba

    def __init__(self, dim: int, predefined_embeddings: dict[str, list[float]] = None):
        self.DIM = dim
        # predefined_embeddings: mapea texto de query/doc a su vector
        self.predefined_embeddings = predefined_embeddings if predefined_embeddings else {}
        # Default embedding si no se encuentra en predefined_embeddings
        self.default_vector = [0.0] * self.DIM

    def embed(self, text: str) -> list[float]:
        return self.predefined_embeddings.get(text, self.default_vector)

# --- Fixture para crear y limpiar artefactos de FAISS para el test ---
@pytest.fixture
def faiss_test_artefacts(tmp_path: Path) -> tuple[Path, Path, dict[str, list[float]], int]:
    dim = 4 # Dimensión pequeña para el test
    num_docs = 3

    # Documentos y sus embeddings predefinidos (simplificados)
    # Estos embeddings están diseñados para que la "query_target_doc1" esté más cerca de "doc1_text"
    doc_embeddings = {
        "doc1_text": [1.0, 0.1, 0.2, 0.3], # Target
        "doc2_text": [0.2, 1.0, 0.3, 0.4],
        "doc3_text": [0.3, 0.2, 1.0, 0.5],
    }
    doc_ids_in_db = [101, 102, 103] # IDs que estarían en la BD

    query_embeddings = {
        "query_target_doc1": [0.9, 0.15, 0.25, 0.35], # Muy similar a doc1_text
        "query_target_doc2": [0.25, 0.9, 0.35, 0.45], # Muy similar a doc2_text
        "query_no_match":    [0.0, 0.0, 0.0, 0.0],   # Diferente a todos
    }

    all_predefined_embeddings = {**doc_embeddings, **query_embeddings}

    # Crear el índice FAISS
    index_path = tmp_path / "test_index.faiss"
    id_map_path = tmp_path / "test_id_map.pkl"

    # Convertir doc_embeddings a numpy array para FAISS
    vectors_np = np.array(list(doc_embeddings.values()), dtype="float32")
    
    # Usar IndexFlatL2 para similaridad de coseno (después de normalizar) o distancia L2
    # Para este ejemplo, IndexFlatL2 (distancia Euclidiana) es más simple.
    # Si los embeddings estuvieran normalizados a longitud unitaria, L2 y producto interno son equivalentes.
    index = faiss.IndexFlatL2(dim)
    index.add(vectors_np)
    faiss.write_index(index, str(index_path))

    # Crear el id_map: mapea índice de FAISS (0, 1, 2) a ID de BD (101, 102, 103)
    id_map_content = {i: doc_id for i, doc_id in enumerate(doc_ids_in_db)}
    with open(id_map_path, "wb") as f:
        pickle.dump(id_map_content, f)

    return index_path, id_map_path, all_predefined_embeddings, dim


# --- Tests ---
def test_faiss_retrieves_closest_doc(faiss_test_artefacts, monkeypatch):
    index_path, id_map_path, predefined_embeddings, dim = faiss_test_artefacts

    # Monkeypatch settings para que el retriever use los artefactos de test
    monkeypatch.setattr(global_settings, "index_path", str(index_path))
    monkeypatch.setattr(global_settings, "id_map_path", str(id_map_path))

    embedder = DummyEmbedder(dim=dim, predefined_embeddings=predefined_embeddings)
    # Nota: DenseFaissRetriever toma una instancia de SentenceTransformerEmbedder en su __init__.
    # Para este test unitario, es mejor si DenseFaissRetriever aceptara un EmbedderPort.
    # Si no, tendremos que mockear SentenceTransformerEmbedder o crear una subclase para el test.
    # ASUMAMOS por ahora que DenseFaissRetriever puede tomar cualquier EmbedderPort.
    # Si no, este es un punto a refactorizar en DenseFaissRetriever.
    
    # Refactor Propuesto para DenseFaissRetriever.__init__:
    # def __init__(self, embedder: EmbedderPort): # En lugar de SentenceTransformerEmbedder
    
    retriever = DenseFaissRetriever(embedder=embedder) # Pasamos nuestro DummyEmbedder

    query_text = "query_target_doc1" # Esta query está diseñada para estar cerca de doc1_text (ID 101)
    
    # Recordar que RetrieverPort ahora devuelve (ids, scores)
    retrieved_ids, retrieved_scores = retriever.retrieve(query_text, k=1)

    assert len(retrieved_ids) == 1
    assert retrieved_ids[0] == 101 # El ID de "doc1_text"
    assert len(retrieved_scores) == 1
    assert isinstance(retrieved_scores[0], float)
    # Para L2, el score (distancia) debería ser pequeño
    assert retrieved_scores[0] < 0.1 # Ajustar este umbral basado en los embeddings de prueba

def test_faiss_respects_k(faiss_test_artefacts, monkeypatch):
    index_path, id_map_path, predefined_embeddings, dim = faiss_test_artefacts
    monkeypatch.setattr(global_settings, "index_path", str(index_path))
    monkeypatch.setattr(global_settings, "id_map_path", str(id_map_path))

    embedder = DummyEmbedder(dim=dim, predefined_embeddings=predefined_embeddings)
    retriever = DenseFaissRetriever(embedder=embedder)

    query_text = "query_target_doc1" # Query que podría coincidir con varios si k es mayor
    k_val = 2
    retrieved_ids, retrieved_scores = retriever.retrieve(query_text, k=k_val)

    assert len(retrieved_ids) == k_val
    assert len(retrieved_scores) == k_val
    assert 101 in retrieved_ids # doc1 debería estar
    # El segundo podría ser doc2 o doc3 dependiendo de las distancias exactas

def test_faiss_no_match(faiss_test_artefacts, monkeypatch):
    index_path, id_map_path, predefined_embeddings, dim = faiss_test_artefacts
    monkeypatch.setattr(global_settings, "index_path", str(index_path))
    monkeypatch.setattr(global_settings, "id_map_path", str(id_map_path))

    embedder = DummyEmbedder(dim=dim, predefined_embeddings=predefined_embeddings)
    retriever = DenseFaissRetriever(embedder=embedder)

    query_text = "query_no_match" # Diseñada para no coincidir bien
    retrieved_ids, retrieved_scores = retriever.retrieve(query_text, k=1)

    assert isinstance(retrieved_ids, list)
    assert isinstance(retrieved_scores, list)
    assert len(retrieved_ids) == len(retrieved_scores)
    if retrieved_ids: # Si devuelve algo
        assert len(retrieved_ids) <=1
        # La distancia para "query_no_match" (vector de ceros) a los otros vectores será mayor
        # que la distancia de "query_target_doc1" a "doc1_text".
        # Puedes añadir un assert sobre el valor del score si es predecible.
        # Por ejemplo, si retrieved_scores[0] > algún umbral alto.


# Más tests posibles:
# - Qué pasa si k es mayor que el número de documentos en el índice.
# - Qué pasa con una query vacía (si el DummyEmbedder la maneja de forma específica).


================================================================================
FILE: tests/unit/test_build.py
================================================================================

# tests/unit/test_build.py

from pathlib import Path
import csv
import importlib
import pickle
import faiss
from src.settings import settings as st
# NO IMPORTAR SessionLocal o Document directamente aquí si van a cambiar con el reload

import src.db.base as db_base_module
import src.db.models as db_models_module
import scripts.build_index as build_index_module

# ------------------------------------------------------------------ #
# helpers
# ------------------------------------------------------------------ #
def _count_csv_rows(csv_path: Path) -> int:
    with csv_path.open(newline="", encoding="utf-8") as fh:
        # Count non-blank lines
        count = 0
        for row in csv.reader(fh):
            if any(field.strip() for field in row): 
                count += 1
        return count


# ------------------------------------------------------------------ #
# TEST 1 – modo sparse (default)
# ------------------------------------------------------------------ #
def test_build_index_sparse(tmp_path, monkeypatch):
    # --- 1) reconfig settings ---
    db_path = tmp_path / "app.db"

    # Don't needed for sparse
    # index_path = tmp_path / "index.faiss"
    # id_map_path = tmp_path / "id_map.pkl"
    # monkeypatch.setattr(st, "index_path", str(index_path))
    # monkeypatch.setattr(st, "id_map_path", str(id_map_path))

    monkeypatch.setattr(st, "sqlite_url", f"sqlite:///{db_path}")
    monkeypatch.setattr(st, "retrieval_mode", "sparse", raising=False)

    # --- 2) recargar engine con nueva URL ---
    importlib.reload(db_base_module)
    importlib.reload(db_models_module)

    # --- 3) ejecutar build_index ---
    build_index_module.main()

    # --- 4) assertions ---
    from src.db.base import SessionLocal
    from src.db.models import Document

    with SessionLocal() as db:
        count_db = db.query(Document).count()

    expected = _count_csv_rows(Path(st.faq_csv)) - 1 # header
    assert count_db == expected, f"Número de filas en DB ({count_db}) debe coincidir con CSV ({expected})"
    assert not (tmp_path / "index.faiss").exists(), "En modo sparse no se genera FAISS" 
    

# ------------------------------------------------------------------ #
# TEST 2 – modo dense
# ------------------------------------------------------------------ #
# ELIMINAR o COMENTAR: @pytest.mark.parametrize("k", [3])
def test_build_index_dense(tmp_path, monkeypatch): # Eliminar ', k'
    # --- 1) reconfig settings ---
    db_path = tmp_path / "app.db"
    index_path = tmp_path / "index.faiss"
    id_map_path = tmp_path / "id_map.pkl"

    monkeypatch.setattr(st, "sqlite_url", f"sqlite:///{db_path}")
    monkeypatch.setattr(st, "index_path", str(index_path))
    monkeypatch.setattr(st, "id_map_path", str(id_map_path))
    monkeypatch.setattr(st, "retrieval_mode", "dense", raising=False)

    # --- 2) recargar engine con nueva URL ---
    importlib.reload(db_base_module)
    importlib.reload(db_models_module)

    # --- 3) ejecutar build_index ---
    build_index_module.main()

    # --- 4) assertions ---
    from src.db.base import SessionLocal
    from src.db.models import Document

    with SessionLocal() as db:
        count_db = db.query(Document).count()
    expected_rows = _count_csv_rows(Path(st.faq_csv)) - 1 # header
    assert count_db == expected_rows, f"Número de filas en DB ({count_db}) debe coincidir con CSV ({expected_rows})"

    assert index_path.exists() and id_map_path.exists(), "FAISS files missing" # Usar variables de path
    faiss_index = faiss.read_index(str(index_path)) # Usar variables de path
    with open(id_map_path, "rb") as fh: # Usar variables de path
        id_map = pickle.load(fh)

    assert faiss_index.ntotal == expected_rows
    assert len(id_map) == expected_rows


================================================================================
FILE: tests/unit/test_rag.py
================================================================================

# tests/unit/test_rag.py
"""
Unit-test del núcleo RagService sin depender de OpenAI ni BM25 reales,
y usando una base de datos en memoria para aislamiento.
"""
import pytest
from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker, Session # Importar Session explícitamente

# Importar Base y modelos directamente para usarlos con el engine en memoria
from src.db.base import Base # El Base original para definir la estructura de tablas
from src.db.models import Document as DbDocument # Alias para evitar confusión con un posible Document de Pydantic

from src.core.rag import RagService
from src.core.ports import RetrieverPort, GeneratorPort # Importar los Puertos

# --- Test Doubles (Stubs) ---
class DummyRetriever(RetrieverPort): # Implementar el Puerto
    def __init__(self, doc_ids: list[int] = None, scores: list[float] = None):
        self.doc_ids_to_return = doc_ids if doc_ids is not None else [1]
        self.scores_to_return = scores if scores is not None else [0.9] * len(self.doc_ids_to_return)
        if len(self.doc_ids_to_return) != len(self.scores_to_return):
            raise ValueError("doc_ids y scores deben tener la misma longitud en DummyRetriever")

    def retrieve(self, query: str, k: int = 5) -> tuple[list[int], list[float]]:
        # Devolver solo hasta k resultados
        return self.doc_ids_to_return[:k], self.scores_to_return[:k]


class DummyGenerator(GeneratorPort): # Implementar el Puerto
    def __init__(self, answer: str = "dummy answer"):
        self.answer_to_return = answer

    def generate(self, question: str, contexts: list[str]) -> str:
        # Podrías incluso hacer asserts sobre 'question' o 'contexts' aquí si fuera necesario
        return self.answer_to_return


# --- Fixture para la base de datos en memoria ---
@pytest.fixture(scope="function") # 'function' scope para una BD limpia por test
def db_session() -> Session: # Tipar el retorno para claridad
    # Crear un engine SQLite en memoria para este test
    engine = create_engine("sqlite:///:memory:")
    # Crear todas las tablas definidas en Base.metadata (ej. Document, QaHistory)
    Base.metadata.create_all(bind=engine)

    # Crear una SessionLocal específica para este engine en memoria
    TestingSessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)
    
    db = TestingSessionLocal()
    try:
        yield db # Proporcionar la sesión al test
    finally:
        db.close()
        Base.metadata.drop_all(bind=engine) # Limpiar después del test


# --- Tests ---
def test_rag_service_ask_retrieves_and_generates(db_session: Session):
    """
    Test que RagService.ask:
    1. Llama a retriever.retrieve().
    2. Llama a crud.get_documents() con los IDs del retriever.
    3. Llama a generator.generate() con la pregunta y los contextos.
    4. Devuelve la respuesta del generador y los IDs fuente.
    """
    # 1. Poblar la BD en memoria con un documento de prueba
    test_doc_id = 1
    test_doc_content = "dummy context content"
    doc = DbDocument(id=test_doc_id, content=test_doc_content)
    db_session.add(doc)
    db_session.commit()

    # 2. Configurar los Test Doubles
    retriever = DummyRetriever(doc_ids=[test_doc_id], scores=[0.95])
    generator = DummyGenerator(answer="specific dummy answer for this test")
    
    rag_service = RagService(retriever, generator)
    
    question = "hello?"

    # Act
    result = rag_service.ask(db_session, question)
    
    # Assert
    assert result["answer"] == "specific dummy answer for this test"
    assert result["source_ids"] == [test_doc_id]
    # Opcional: verificar que el historial se guardó (si QaHistory está definido en Base.metadata)
    # from src.db.models import QaHistory
    # history_entry = db_session.query(QaHistory).first()
    # assert history_entry is not None
    # assert history_entry.question == question
    # assert history_entry.answer == "specific dummy answer for this test"


def test_rag_service_ask_no_documents_found(db_session: Session):
    """
    Test que RagService.ask maneja el caso donde el retriever no devuelve IDs
    o los IDs no corresponden a documentos en la BD.
    El generador debería ser llamado con un contexto vacío.
    """
    # Arrange
    # BD está vacía (o los IDs devueltos no existen)
    
    retriever = DummyRetriever(doc_ids=[], scores=[]) # Retriever no devuelve nada
    
    # Queremos asegurar que el generador se llama con contextos vacíos
    class GeneratorSpy(GeneratorPort):
        called_with_contexts: list[str] | None = None
        def generate(self, question: str, contexts: list[str]) -> str:
            self.called_with_contexts = contexts
            return "generated with no context"

    generator_spy = GeneratorSpy()
    rag_service = RagService(retriever, generator_spy)
    question = "any question"

    # Act
    result = rag_service.ask(db_session, question)

    # Assert
    assert result["answer"] == "generated with no context"
    assert result["source_ids"] == []
    assert generator_spy.called_with_contexts == [] # Verifica que el contexto estaba vacío


def test_rag_service_uses_k_for_retrieval(db_session: Session):
    """
    Test que RagService.ask pasa el parámetro 'k' (o su default) al retriever.
    """
    # Arrange
    # Poblar la BD con suficientes documentos para que 'k' importe
    for i in range(1, 6):
        db_session.add(DbDocument(id=i, content=f"doc {i}"))
    db_session.commit()

    class RetrieverSpy(RetrieverPort):
        called_with_k: int | None = None
        def retrieve(self, query: str, k: int = 5) -> tuple[list[int], list[float]]:
            self.called_with_k = k
            # Devolver algunos IDs y scores válidos para que el flujo continúe
            actual_ids = [1, 2, 3, 4, 5]
            return actual_ids[:k], [0.9] * k 

    retriever_spy = RetrieverSpy()
    generator = DummyGenerator() # No nos importa la respuesta aquí
    rag_service = RagService(retriever_spy, generator)
    
    # Act
    rag_service.ask(db_session, "test question", k=3) # Llamar con k=3

    # Assert
    assert retriever_spy.called_with_k == 3

    # Act: Llamar sin k explícito (debería usar el default de RagService.ask, que es 3)
    rag_service.ask(db_session, "another test question")
    assert retriever_spy.called_with_k == 3 # El default de RagService.ask es k=3.

